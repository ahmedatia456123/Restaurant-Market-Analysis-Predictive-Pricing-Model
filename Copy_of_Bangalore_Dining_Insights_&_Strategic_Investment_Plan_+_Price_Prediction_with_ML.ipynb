{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedatia456123/Restaurant-Market-Analysis-Predictive-Pricing-Model/blob/main/Copy_of_Bangalore_Dining_Insights_%26_Strategic_Investment_Plan_%2B_Price_Prediction_with_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK--CMnu0gUY"
      },
      "source": [
        "# Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdqbNeQi26fW"
      },
      "source": [
        "## Introduction to Analyzing the Zomato Dataset\n",
        "\n",
        "Analyzing the Zomato dataset offers valuable insights into the restaurant scene in Bengaluru, a city bustling with over 12,000 eateries that cater to a diverse range of culinary tastes from around the world. As new restaurants continue to open daily, the industry remains dynamic, with growing demand that presents both opportunities and challenges. For newcomers, competing with well-established establishments can be tough, especially when many restaurants offer similar fare.\n",
        "\n",
        "Bengaluru, known as the IT hub of India, has a large population that relies heavily on dining out due to busy lifestyles, making the study of restaurant demographics crucial. This analysis aims to uncover key patterns and preferences, including:\n",
        "\n",
        "- **Popularity of Various Cuisines**: Identifying which types of food are favored in different localities.\n",
        "- **Vegetarian Preferences**: Examining if certain areas show a strong inclination towards vegetarian dishes and whether these areas are predominantly inhabited by specific communities, such as Jains, Marwaris, or Gujaratis.\n",
        "- **Restaurant Characteristics**: Evaluating factors such as the restaurant's location, pricing, and whether it follows a theme.\n",
        "- **Local Cuisine Trends**: Determining which neighborhoods are renowned for particular types of cuisine and the factors driving these preferences.\n",
        "\n",
        "By studying these aspects, we can gain a deeper understanding of the restaurant landscape in Bengaluru, helping new and existing restaurants better align with local tastes and demands.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ931FXO0k71"
      },
      "source": [
        "# Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLuhxEMi6OEa"
      },
      "source": [
        "The primary objective of this data analysis project is to identify the most promising investment opportunities in the restaurant and cafe sector in Bangalore. This involves analyzing various factors that influence the success and customer appeal of these establishments, and developing machine learning models to support pricing strategies and enhance customer experience.\n",
        "\n",
        "**Key Goals:**\n",
        "\n",
        "1. **Investment Analysis:**\n",
        "   - **Identify High-Performing Establishments:** Analyze the data to pinpoint restaurants and cafes with high ratings, significant customer engagement, and strong financial performance indicators. Focus on key attributes such as location, type, and customer reviews to assess which establishments are likely to offer the best returns on investment.\n",
        "   - **Evaluate Pricing Strategies:** Develop and implement machine learning models to predict optimal pricing for menu items based on factors such as location, type, and customer feedback. This will help establish competitive pricing that aligns with market expectations and maximizes profitability.\n",
        "\n",
        "2. **Customer Experience Enhancement:**\n",
        "   - **Analyze Customer Preferences:** Utilize the data to understand customer preferences regarding dish likes, cuisines, and other attributes. This will inform strategies to improve the dining experience by focusing on popular dishes, preferred cuisines, and services that enhance overall satisfaction.\n",
        "   - **Improve Engagement and Accessibility:** Examine the impact of online ordering and table booking options on customer engagement and satisfaction. Determine how these features contribute to higher ratings and increased customer interactions.\n",
        "\n",
        "3. **Machine Learning Model Development:**\n",
        "   - **Predictive Pricing Model:** Build and refine machine learning models to forecast prices for menu items based on historical data, restaurant type, location, and customer reviews. This model will provide insights into setting competitive prices that attract customers while ensuring profitability.\n",
        "   - **Enhancement Recommendations:** Generate actionable recommendations for improving customer experience based on predictive analytics and historical trends. This will include suggestions for menu adjustments, service enhancements, and strategic changes to attract and retain customers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5xLx1gX0m-t"
      },
      "source": [
        "# libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKP67Y9O6Rhm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected=False)\n",
        "from wordcloud import WordCloud\n",
        "from geopy.geocoders import Nominatim\n",
        "from folium.plugins import HeatMap\n",
        "import folium\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import word2vec\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9fgLmsX6ZLX"
      },
      "outputs": [],
      "source": [
        "!pip install geopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZTuvx15YII5"
      },
      "outputs": [],
      "source": [
        "!pip install kmodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_13oEhk6TTp"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgeSq0rl6cQH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey36HBS26i4K"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Zomato Geospatial Analysis/zomato.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxOHYjgv0t52"
      },
      "source": [
        "# Cleaning & Preparing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm72a8426lmY"
      },
      "outputs": [],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6_8CWbM6o6H"
      },
      "outputs": [],
      "source": [
        "print(f'shape of the data {df.shape[0]} rows and {df.shape[1]} columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDpU1Oye6r4o"
      },
      "outputs": [],
      "source": [
        "# Checking columns dtypes\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M8-I6BK6yVw"
      },
      "source": [
        "We have a problem with the `rate` and `approx_cost` columns. The `rate` column should be in float format, while the `approx_cost` column should be in integer format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA5xgUjB0yqP"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmGki1t706hf"
      },
      "source": [
        "### Dealing with dublicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrC0tNCu66oC"
      },
      "outputs": [],
      "source": [
        "#checking duplication\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJBC8Lix0-mU"
      },
      "source": [
        "### Dealing with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0j7N7Wz691f"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SMI7Ybl_FuC"
      },
      "outputs": [],
      "source": [
        "# First thing i wanna do is removing query string from link\n",
        "df.url = df.url.apply(lambda x: x.split('?')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-8xki5G_h0e"
      },
      "source": [
        "After reviewing the data, I noticed that there are multiple recordings for some restaurants over time. These recordings are not very useful because there are very few of them per restaurant, and they only show an increase in the number of votes and changes in the order of liked dishes without any other significant differences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHaPt2OsAu-b"
      },
      "outputs": [],
      "source": [
        "# Taking a copy\n",
        "original_df = df.copy(deep=True)\n",
        "# I will sort them by votes then group them by url and get last row of each group to get latest recored\n",
        "df = df.sort_values('votes').groupby('address').last().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoH70jVWFCiJ"
      },
      "outputs": [],
      "source": [
        "df[df['name']=='Cafe Coffee Day'].url.iloc[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDflBjdh7Pfa"
      },
      "outputs": [],
      "source": [
        "# Checking location missing values\n",
        "df[df['location'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPpHsex97bfb"
      },
      "source": [
        "I could fill the `location` field using information from the `listed_in (city)` or `address` columns. However, I will remove this field as it contains missing values in most of the records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH9Z6Srj7CNp"
      },
      "outputs": [],
      "source": [
        "# remove location with null values\n",
        "df.dropna(subset=['location'],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaMV5HML7z8y"
      },
      "source": [
        "Dealing with rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSmsgPdm9o4N"
      },
      "outputs": [],
      "source": [
        "print(f'Percentage of miss values form rating column {df[\"rate\"].isnull().sum()/df.shape[0]*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6FtgmM798HF"
      },
      "source": [
        "A 15% missing value rate is too high to delete the affected rows. Instead, I will find a way to fill these missing values with meaningful data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLhBwAn9737b"
      },
      "outputs": [],
      "source": [
        "#Check what values are in rates\n",
        "df['rate'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piFoOAfM8CwU"
      },
      "source": [
        "I need the ratings to be in numeric format. However, the current data contains `Null` values, as well as entries marked as `'NEW'` and `'-'`. To address this:\n",
        "\n",
        "1. **Calculate Ratings from Reviews:** I will derive ratings from the `reviews_list` column where available.\n",
        "   \n",
        "2. **Handle Missing Ratings:** For restaurants with no reviews, I will use the average rating of restaurants in the same location to estimate their ratings.\n",
        "\n",
        "3. **Preserve 'NEW' Information:** To retain the information about new establishments, I will create a new column named `is_new` with values `'yes'` or `'no'`. This will facilitate visualization. Later, for modeling purposes, this column will be converted to binary values (`1` and `0`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jup8LJm785K"
      },
      "outputs": [],
      "source": [
        "# First Create the is_new\n",
        "df['is_new'] = df.rate.apply(lambda x: 'Yes' if x=='NEW' else 'No')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo3TTQpE-bxX"
      },
      "outputs": [],
      "source": [
        "print(f'Number of new establishments {df.is_new.value_counts()[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_S5xMSa-uTF"
      },
      "outputs": [],
      "source": [
        "print(f'NEW represent {100*2208/df.shape[0]}% of the data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33l6fGVX-P7V"
      },
      "outputs": [],
      "source": [
        "# Change `None` values to `-` to make them easier to handle.\n",
        "\n",
        "df.rate = df.rate.fillna('-')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-znO3j2r-069"
      },
      "source": [
        "I will now prepare the `review_list` column by changing its format from a string to a list. This will allow me to work with the data more effectively and extract useful information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pEUnfqd_FuX"
      },
      "outputs": [],
      "source": [
        "# Fix review_list column and pase it into python list\n",
        "df.reviews_list = df.reviews_list.apply(lambda x: eval(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa6DdKRY_jun"
      },
      "outputs": [],
      "source": [
        "def get_rating_for_NEW_res(row):\n",
        "    # Check if the rate is neither 'NEW' nor '-'. If it's a valid rate, return it.\n",
        "    if row.rate != 'NEW' and row.rate != '-':\n",
        "        return row.rate\n",
        "\n",
        "    # Retrieve the list of reviews from the row\n",
        "    reviews_list = row.reviews_list\n",
        "\n",
        "    # If there are no reviews available, return 'NEW' to indicate the restaurant is new\n",
        "    if len(reviews_list) == 0:\n",
        "        return 'NEW'\n",
        "\n",
        "    # Initialize a list to hold the numeric ratings extracted from the reviews\n",
        "    rating_ph = []\n",
        "\n",
        "    # Iterate through each review in the reviews_list\n",
        "    for review in reviews_list:\n",
        "        # Skip reviews with None or invalid rating values\n",
        "        if review[0] is None:\n",
        "            continue\n",
        "\n",
        "        # Extract and clean the rating value from the review string\n",
        "        rate = float(review[0].lower().replace('rated', ''))\n",
        "        rating_ph.append(rate)\n",
        "\n",
        "    # If no valid ratings were extracted, return 'NEW'\n",
        "    if len(rating_ph) == 0:\n",
        "        return 'NEW'\n",
        "\n",
        "    # Calculate the average rating from the extracted ratings and return it formatted as a string\n",
        "    return f'{\"{:.2f}\".format(np.mean(rating_ph))}/5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxUC3rW_Dt98"
      },
      "outputs": [],
      "source": [
        "df.rate = df.apply(lambda row: get_rating_for_NEW_res(row), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBxoKoeZD2ar"
      },
      "outputs": [],
      "source": [
        "#Check Number of new again\n",
        "len(df[df.rate=='NEW'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RQuoiREEBqh"
      },
      "source": [
        "Now, I will apply a new function to calculate the average rating for restaurants in a city that have a 'NEW' value. First, I will convert the rate column to float format after obtaining the rating as a float number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4X385nHERB5"
      },
      "outputs": [],
      "source": [
        "df['rate'] = df['rate'].apply(lambda x: float(x.split('/')[0]) if x != 'NEW' else x)\n",
        "\n",
        "avg_by_location = df[df.rate!='NEW'][['location','rate']].groupby('location').median()\n",
        "def get_avg_rateing_for_NEW_res(row):\n",
        "    \"\"\"\n",
        "    Function to get the average rating for restaurants with 'NEW' status.\n",
        "\n",
        "    Parameters:\n",
        "    row (pd.Series): A row from the DataFrame containing restaurant details.\n",
        "\n",
        "    Returns:\n",
        "    float or None: The rating of the restaurant if not 'NEW', or the average rating for the location if 'NEW'.\n",
        "    \"\"\"\n",
        "    # Check if the rate is not 'NEW'\n",
        "    if row.rate != 'NEW':\n",
        "        return row.rate\n",
        "\n",
        "    # Retrieve the location of the restaurant\n",
        "    loc = row.location\n",
        "\n",
        "    # Check if the location exists in the average ratings by location index\n",
        "    if loc not in avg_by_location.index:\n",
        "        return None\n",
        "\n",
        "    # Return the average rating for the location\n",
        "    return avg_by_location.loc[loc].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVsjtRI7EUXP"
      },
      "outputs": [],
      "source": [
        "df.rate = df.apply(lambda row:get_avg_rateing_for_NEW_res(row),axis=1)\n",
        "# The remaining values represent a small percentage and cannot be engineered meaningfully, so I will remove them.\n",
        "df.dropna(subset=['rate'],inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3YRJUywH8U9"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['cuisines'],inplace=True)\n",
        "df.dropna(subset=['rest_type'],inplace=True)\n",
        "df.dropna(subset=['approx_cost(for two people)'],inplace=True)\n",
        "# now we have only phone and i dont need it so i will remove it\n",
        "df.drop(columns=['phone'],inplace=True)\n",
        "df['approx_cost(for two people)'] = df['approx_cost(for two people)'].apply(lambda x: int(x.replace(',','')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHqPujio1FJ3"
      },
      "source": [
        "### Features Enginnering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7b85J8uHSZF"
      },
      "source": [
        "**Dealing with resturant types**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjVe9aotHTiu"
      },
      "outputs": [],
      "source": [
        "#checking resturant type values\n",
        "df.rest_type.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAlQFBjFHgiW"
      },
      "source": [
        "Some restaurants have multiple types. I will extract and separate these types for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFY6qWZJHjBX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def separate_types(types,lower=False):\n",
        "    \"\"\"\n",
        "    Function to extract and separate multiple types from a single column into individual columns.\n",
        "\n",
        "    Parameters:\n",
        "    types (str): The name of the column containing types.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Reset index of the DataFrame to ensure sequential indexing\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Initialize a variable to track the maximum number of types for any restaurant\n",
        "    max_types = 1\n",
        "\n",
        "    # Iterate through each row of the DataFrame\n",
        "    for i, row in df.iterrows():\n",
        "        # Check if there are multiple types separated by commas\n",
        "        if ',' in row[types]:\n",
        "            # Split the types by comma and strip any extra whitespace\n",
        "            list_type = [x.strip() for x in row[types].split(',')]\n",
        "            # Update the cell with the list of types\n",
        "            df.at[i, types] = list_type\n",
        "            # Update the maximum number of types if this row has more\n",
        "            if len(list_type) > max_types:\n",
        "                max_types = len(list_type)\n",
        "        else:\n",
        "            # If only one type, store it as a list with a single element\n",
        "            df.loc[i, types] = [row[types]]\n",
        "\n",
        "    # Print the maximum number of types for a single restaurant\n",
        "    print(f'Max {types} for single restaurants is {max_types}')\n",
        "\n",
        "    # Iterate through each row and list of types\n",
        "    for indx, t_list in enumerate(df[types].tolist()):\n",
        "        list_len = len(t_list)\n",
        "        # Iterate through each type in the list\n",
        "        for j in range(list_len):\n",
        "            # Create a new column name for each type\n",
        "            column_name = types + '_' + str(j)\n",
        "            # Add the new column if it doesn't exist\n",
        "            if column_name not in df.columns:\n",
        "                df[column_name] = '-'\n",
        "            # Update the DataFrame with the type value\n",
        "            if lower:\n",
        "              text = re.sub(r'\\b\\d+(?!th|nd|rd)\\b', '', t_list[j].lower().replace('bangalore',''))\n",
        "              text = cleaned_text = re.sub(r'\\W+', '', text)\n",
        "              df.at[indx, column_name] = text\n",
        "            else:\n",
        "              df.at[indx, column_name] = t_list[j]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6bqXpcJHtje"
      },
      "outputs": [],
      "source": [
        "separate_types('rest_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2uWbOQhJgK2"
      },
      "source": [
        "Dealing With Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8gRVKqYEuhI"
      },
      "outputs": [],
      "source": [
        "sns.displot(df.rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLCphRmLE0EX"
      },
      "source": [
        "We have other problem some resturants has rate 5 of 5 but only has one vote this is misleading and gonna effect our model se we need to take Weighted_rating\n",
        "### Weighted Rating Formula\n",
        "\n",
        "The Bayesian average formula for calculating a weighted rating is:\n",
        "\n",
        "$$\n",
        "\\text{Weighted Rating} = \\frac{v \\cdot r + m \\cdot c}{v + m}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\( r \\) is the average rating of the item.\n",
        "- \\( v \\) is the number of votes for the item.\n",
        "- \\( m \\) is the minimum number of votes required to be listed (threshold).\n",
        "- \\( c \\) is the mean rating across all items.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "- **\\( v \\cdot r \\)**: The total score for the item based on its own ratings.\n",
        "- **\\( m \\cdot c \\)**: A balancing factor that brings in the average of all ratings, weighted by the minimum number of votes required.\n",
        "- **\\( v + m \\)**: The total weight, combining both the item's votes and the overall system's expectations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp0XewzAEzwh"
      },
      "outputs": [],
      "source": [
        "overall_avg = df.rate.mean()\n",
        "min_votes = 20\n",
        "print(f'Mean Rating = {overall_avg}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0Uez1UeE_XS"
      },
      "outputs": [],
      "source": [
        "def calculate_weighted_rating(row, overall_avg, min_votes):\n",
        "    v = row['votes']\n",
        "    r = row['rate']\n",
        "    c = overall_avg\n",
        "    m = min_votes\n",
        "\n",
        "    # Calculate the weighted rating\n",
        "    weighted_rating = (v * r + m * c) / (v + m)\n",
        "    return round(weighted_rating,2)\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df['weighted_rating'] = df.apply(lambda row: calculate_weighted_rating(row, overall_avg, min_votes), axis=1)\n",
        "\n",
        "# if there is a change by 0.5 to rate due to averaging then i will marke that rate as unvalid\n",
        "df['is_rate_valid'] = df.apply(lambda row: 1 if abs(row.rate-row.weighted_rating) < 0.5 else 0, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVVI7FmvFKvy"
      },
      "outputs": [],
      "source": [
        "df.is_rate_valid.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn62jUddXIJM"
      },
      "source": [
        "Dealing with `menu_item`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yv0FgLuVVtH"
      },
      "outputs": [],
      "source": [
        "# Converting menu_item into list\n",
        "df.menu_item = df.menu_item.apply(lambda x: eval(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FU_ebNyJo5s"
      },
      "source": [
        "Dealing with cuisines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuVCBvJOJpvP"
      },
      "outputs": [],
      "source": [
        "df.cuisines.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqYNUnwrJsv8"
      },
      "outputs": [],
      "source": [
        "separate_types('cuisines')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G4tPMYcJ3jb"
      },
      "source": [
        "Dealing with dish_liked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVCoI26EJ5Yb"
      },
      "outputs": [],
      "source": [
        "# i will change value with none for dish_liked to -\n",
        "df.dish_liked = df.dish_liked.fillna('-')\n",
        "separate_types('dish_liked')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf0Ctsm7LkeS"
      },
      "outputs": [],
      "source": [
        "# Add column to know if the location is on Road not not\n",
        "df['is_road'] = df.location.apply(lambda x: 'Yes' if 'Road' in x else 'No')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8QyW3BIJljB"
      },
      "source": [
        "Dealing with location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJLIyMY2Jk2U"
      },
      "outputs": [],
      "source": [
        "location_df = df.location.value_counts().reset_index()\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"app\" , timeout=None ) ## set timeout=None to get rid of timeout error\n",
        "\n",
        "lat = [] ## define lat list to store all the latitudes\n",
        "lon = [] ## define lon list to store all the longitudes\n",
        "\n",
        "\n",
        "for name in location_df.location.tolist():\n",
        "    location = geolocator.geocode(name+', Bangalore',country_codes='IN')\n",
        "\n",
        "    if location is None:\n",
        "        lat.append(np.nan)\n",
        "        lon.append(np.nan)\n",
        "\n",
        "    else:\n",
        "        lat.append(location.latitude)\n",
        "        lon.append(location.longitude)\n",
        "\n",
        "location_df['lat'] = lat\n",
        "location_df['lon']=lon\n",
        "location_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO3A0Fk3LIwH"
      },
      "outputs": [],
      "source": [
        "#fixing the two missing locations\n",
        "location_df[location_df.lat.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6jKK-R6LQJU"
      },
      "outputs": [],
      "source": [
        "index = location_df[location_df.lat.isna()].index[0]\n",
        "location_df.loc[index,'lat'] = 13.0080\n",
        "location_df.loc[index,'lon'] = 77.5800\n",
        "index = location_df[location_df.lat.isna()].index[0]\n",
        "location_df.loc[index,'lat'] = 13.0827\n",
        "location_df.loc[index,'lon'] = 77.6785"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh2i-u_DM8iy"
      },
      "outputs": [],
      "source": [
        "#Merging to main df\n",
        "df = df.merge(location_df,on='location',how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "409Hl04TVA0F"
      },
      "outputs": [],
      "source": [
        "# Number of Specialization per resturant\n",
        "df['num_spec'] = df.rest_type.apply(lambda x: len(x))\n",
        "# Number of Liked Dishes per resturant\n",
        "df['num_dish_liked'] = df.dish_liked.apply(lambda x: len(x))\n",
        "# Number of Liked Dishes per resturant\n",
        "df['num_reviews'] = df.reviews_list.apply(lambda x: len(x))\n",
        "# Number of menu item per resturant\n",
        "df['num_menu_item'] = df.menu_item.apply(lambda x: len(x))\n",
        "# Number of cuisines per resturant\n",
        "df['num_cuisines'] = df.cuisines.apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9GEkb9ARM7K"
      },
      "source": [
        "Encoding Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQiMwcYMRXU6"
      },
      "source": [
        "Columns need to be deleted because they do not add value to the prediction model, or there are alternative features that were engineered earlier, would cause multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8xKFeDjROzh"
      },
      "outputs": [],
      "source": [
        "cols = ['url','address','name','rate','location','rest_type','dish_liked','cuisines','reviews_list','listed_in(city)','menu_item','count']\n",
        "encoded_df = df.drop(cols,axis=1).copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dRC2AxTS_F2"
      },
      "source": [
        "Binary encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VlHVnHwS9e3"
      },
      "outputs": [],
      "source": [
        "encoded_df['online_order'] = encoded_df['online_order'].replace({'Yes': 1, 'No': 0})\n",
        "encoded_df['book_table'] = encoded_df['book_table'].replace({'Yes': 1, 'No': 0})\n",
        "encoded_df['is_new'] = encoded_df['is_new'].replace({'Yes': 1, 'No': 0})\n",
        "encoded_df['is_road'] = encoded_df['is_road'].replace({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDS8JGZ3TJgt"
      },
      "source": [
        "Target Encoding With Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MtJWuBORp_i"
      },
      "outputs": [],
      "source": [
        "def target_encode_multiple_columns_with_smoothing(columns, df, target_col, ignore_value='-', smoothing=1):\n",
        "    \"\"\"\n",
        "    Performs target encoding with smoothing on a list of specified columns of the DataFrame,\n",
        "    replacing the specified ignore value with None (NaN). The original columns are updated\n",
        "    with the encoded values, while None values are left unchanged.\n",
        "\n",
        "    Parameters:\n",
        "    columns (list of str): The list of column names to encode.\n",
        "    df (pd.DataFrame): The DataFrame containing the data.\n",
        "    target_col (str): The name of the target column.\n",
        "    ignore_value (str): The value to ignore and replace with None (default is '-').\n",
        "    smoothing (int): The smoothing factor to balance between category mean and global mean.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with the original columns updated with the smoothed encoded values,\n",
        "                  and ignored values replaced with None.\n",
        "    dict: Dictionary of dictionaries containing the feature values used in the encoding for each column,\n",
        "          for mapping to test sets.\n",
        "    \"\"\"\n",
        "    feature_mappings = {}\n",
        "\n",
        "    for column_name in columns:\n",
        "        # Ensure the column exists in the DataFrame\n",
        "        if column_name not in df.columns or target_col not in df.columns:\n",
        "            raise ValueError(f\"Column '{column_name}' or '{target_col}' does not exist in the DataFrame.\")\n",
        "\n",
        "        # Replace the ignore_value with None (which will be treated as NaN in Pandas)\n",
        "        df[column_name] = df[column_name].replace(ignore_value, None)\n",
        "\n",
        "        # Calculate the global mean of the target column\n",
        "        global_mean = df[target_col].mean()\n",
        "\n",
        "        # Calculate the mean and count for each category, excluding NaN values\n",
        "        agg = df[df[column_name].notna()].groupby(column_name)[target_col].agg(['mean', 'count'])\n",
        "\n",
        "        # Calculate the smoothed values using the formula:\n",
        "        # smoothed_value = (mean * count + global_mean * smoothing) / (count + smoothing)\n",
        "        agg['smoothed'] = (agg['mean'] * agg['count'] + global_mean * smoothing) / (agg['count'] + smoothing)\n",
        "\n",
        "        # Store the smoothed values in the feature_mappings dictionary\n",
        "        smoothed_values_dict = agg['smoothed'].to_dict()\n",
        "        feature_mappings[column_name] = smoothed_values_dict\n",
        "\n",
        "        # Map the smoothed values to the DataFrame, leaving None values unchanged\n",
        "        df[column_name] = df[column_name].apply(lambda x: smoothed_values_dict[x] if pd.notna(x) else None)\n",
        "\n",
        "    return df, feature_mappings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRZOcsyIRvwO"
      },
      "outputs": [],
      "source": [
        "cols = ['rest_type_0','rest_type_1','cuisines_0','cuisines_1','cuisines_2','cuisines_3','cuisines_4','cuisines_5','cuisines_6','cuisines_7','dish_liked_0','dish_liked_1','dish_liked_2','dish_liked_3','dish_liked_4','dish_liked_5','dish_liked_6','listed_in(type)']\n",
        "encoded_df, feature_mappings = target_encode_multiple_columns_with_smoothing(cols, encoded_df,'approx_cost(for two people)', smoothing=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLEH-k6XSArj"
      },
      "outputs": [],
      "source": [
        "def sum_columns_and_delete(df, columns, new_column_name):\n",
        "    \"\"\"\n",
        "    Sums the specified columns into a new column and deletes the original columns.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame containing the data.\n",
        "    columns (list of str): The list of column names to sum.\n",
        "    new_column_name (str): The name of the new column where the sum will be stored.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The DataFrame with the new summed column and original columns deleted.\n",
        "    \"\"\"\n",
        "    # Ensure all columns exist in the DataFrame\n",
        "    for column in columns:\n",
        "        if column not in df.columns:\n",
        "            raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Sum the specified columns into a new column\n",
        "    df[new_column_name] = df[columns].sum(axis=1)\n",
        "\n",
        "    # Drop the original columns\n",
        "    df = df.drop(columns=columns)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIsqgoEeSual"
      },
      "outputs": [],
      "source": [
        "cols=['rest_type_0','rest_type_1']\n",
        "encoded_df = sum_columns_and_delete(encoded_df,cols,'rest_type')\n",
        "\n",
        "cols=['cuisines_0','cuisines_1','cuisines_2','cuisines_3','cuisines_4','cuisines_5','cuisines_6','cuisines_7']\n",
        "encoded_df = sum_columns_and_delete(encoded_df,cols,'cuisines')\n",
        "\n",
        "cols=['dish_liked_0','dish_liked_1','dish_liked_2','dish_liked_3','dish_liked_4','dish_liked_5','dish_liked_6']\n",
        "encoded_df = sum_columns_and_delete(encoded_df,cols,'dish_liked')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHVqd42TSw6W"
      },
      "outputs": [],
      "source": [
        "encoded_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDgHWJSx1eVp"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhyz-DZhMQsd"
      },
      "outputs": [],
      "source": [
        "types_frq = None\n",
        "# @title\n",
        "def min_max_scaling(data, new_min=0, new_max=3500):\n",
        "    \"\"\"\n",
        "    Function to perform min-max scaling on a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (array-like): Input data to be scaled.\n",
        "    new_min (float): The new minimum value of the scaled data. Default is 0.\n",
        "    new_max (float): The new maximum value of the scaled data. Default is 3500.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Scaled data with values between new_min and new_max.\n",
        "    \"\"\"\n",
        "    # Ensure that data is a numpy array\n",
        "    data = np.array(data)\n",
        "\n",
        "    # Compute the minimum and maximum of the original data\n",
        "    original_min = np.min(data)\n",
        "    original_max = np.max(data)\n",
        "\n",
        "    # Apply min-max scaling formula to rescale the data\n",
        "    scaled_data = ((data - original_min) / (original_max - original_min)) * (new_max - new_min) + new_min\n",
        "    return scaled_data\n",
        "\n",
        "def multi_bar_chart(temp_df, chart_title, axis_title, figuresize=(15, 6), types_frq=types_frq, freq=True):\n",
        "    \"\"\"\n",
        "    Function to create a multi-bar chart with a line plot overlay.\n",
        "\n",
        "    Parameters:\n",
        "    temp_df (pd.DataFrame): DataFrame containing data to be plotted.\n",
        "    chart_title (str): Title of the chart.\n",
        "    axis_title (str): Title of the x-axis.\n",
        "    figuresize (tuple): Size of the figure. Default is (15, 6).\n",
        "    types_frq (pd.DataFrame): DataFrame with frequency data for the line plot.\n",
        "    freq (bool): Flag to determine if the frequency line plot should be included. Default is True.\n",
        "\n",
        "    Returns:\n",
        "    None: Displays the chart.\n",
        "    \"\"\"\n",
        "    # Create a figure and an axis for the primary y-axis\n",
        "    fig, ax1 = plt.subplots(figsize=figuresize)\n",
        "\n",
        "    # Plot the bar plot for 'approx_cost(for two people)' on the primary y-axis\n",
        "    sns.barplot(x=temp_df.index, y=temp_df['approx_cost(for two people)'], ax=ax1, color='blue', alpha=0.8, label='Approximate Cost')\n",
        "\n",
        "    # Plot the bar plot for 'votes' on the primary y-axis\n",
        "    sns.barplot(x=temp_df.index, y=temp_df['votes'], ax=ax1, color='orange', alpha=0.8, label='Votes')\n",
        "\n",
        "    # Optionally plot the scaled frequency line if 'freq' is True\n",
        "    if freq:\n",
        "        scaled_freq = min_max_scaling(types_frq['count'], new_max=temp_df.votes.max())\n",
        "        sns.lineplot(x=types_frq[types_frq.columns[0]], y=scaled_freq, ax=ax1, color='black', label='Scaled Frequency')\n",
        "\n",
        "    # Set x-axis label and primary y-axis label, and customize tick parameters\n",
        "    ax1.set_xlabel(axis_title)\n",
        "    ax1.set_ylabel('Median (Cost and Votes)', color='blue')\n",
        "    ax1.tick_params(axis='y', labelcolor='blue')\n",
        "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
        "\n",
        "    # Create a secondary y-axis for the line plot\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the line plot for 'weighted_rating' on the secondary y-axis\n",
        "    sns.lineplot(x=temp_df.index, y=temp_df['weighted_rating'], ax=ax2, color='red', label='Weighted Rating')\n",
        "\n",
        "    # Set secondary y-axis label and customize tick parameters\n",
        "    ax2.set_ylabel('Weighted Rating', color='red')\n",
        "    ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "    # Add legends for both y-axes\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    # Set the title of the chart and display it\n",
        "    plt.title(chart_title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def pie_chart(df, column_name,title,figsize=[10,10]):\n",
        "  sns.set(style=\"whitegrid\")\n",
        "  count = df[column_name].value_counts()\n",
        "  plt.figure(figsize=(figsize[0], figsize[1]))\n",
        "  plt.pie(count, labels=count.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette(\"pastel\"))\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def count_chart_with_percentage(types_frq, column_name, title,x_title,range_list=None):\n",
        "  # Calculate cumulative sum and percentage\n",
        "  types_frq['cumsum'] = types_frq['count'].cumsum()\n",
        "  types_frq['cumperc'] = 100 * types_frq['cumsum'] / types_frq['count'].sum()\n",
        "  if range_list is not None:\n",
        "    types_frq = types_frq[range_list[0]:range_list[1]]\n",
        "  # Create a figure with two subplots in one row\n",
        "  # Plotting\n",
        "  fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "  # Bar plot for counts\n",
        "  sns.barplot(x=types_frq[column_name], y=types_frq['count'], color='b', ax=ax1, label='Count')\n",
        "\n",
        "  # Plot cumulative sum\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.plot(types_frq[column_name], types_frq['cumperc'], color='r', marker='o', label='Cumulative Percentage')\n",
        "  ax2.set_ylabel('Cumulative Percentage')\n",
        "\n",
        "  # Add labels and title\n",
        "  ax1.set_title(title)\n",
        "  ax1.set_xlabel(x_title)\n",
        "  ax1.set_ylabel('Count')\n",
        "  ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
        "  ax1.legend(loc='upper left')\n",
        "  ax2.legend(loc='upper right')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def create_radar_chart(ax, data, categories, color,single_chart, label):\n",
        "    \"\"\"\n",
        "    Creates a radar chart on the given axis.\n",
        "\n",
        "    :param ax: Matplotlib axis to plot on.\n",
        "    :param data: Series containing data values.\n",
        "    :param categories: List of category labels.\n",
        "    :param color: Color for the radar chart.\n",
        "    :param label: Label for the legend.\n",
        "    \"\"\"\n",
        "    num_vars = len(categories)\n",
        "\n",
        "    # Compute angle for each category\n",
        "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "    angles += angles[:1]  # Complete the loop\n",
        "\n",
        "    # Append the start value to the end to close the circle\n",
        "    data = data.tolist()\n",
        "    data += data[:1]\n",
        "\n",
        "    # Create the radar chart\n",
        "    if single_chart:\n",
        "      ax.fill(angles, data, color=color, alpha=0.05)\n",
        "    else:\n",
        "      ax.plot(angles, data, color=color, alpha=0.25)\n",
        "\n",
        "    ax.plot(angles, data, color=color, linewidth=2, label=label)\n",
        "\n",
        "    # Set the labels and title\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(categories, rotation=45)\n",
        "    ax.set_title(label, size=15, color=color, y=1.1)\n",
        "\n",
        "def plot_radar_charts(df, single_chart=False, custom_title='Characteristics',scalling=True):\n",
        "    \"\"\"\n",
        "    Generates and displays radar charts for each row of a DataFrame.\n",
        "\n",
        "    :param df: DataFrame where each row represents data for a radar chart.\n",
        "    :param single_chart: Boolean to decide if all data should be plotted in one radar chart or in a grid.\n",
        "    :param custom_title: Custom title for the single radar chart.\n",
        "    \"\"\"\n",
        "    categories = df.columns.tolist()\n",
        "    if scalling:\n",
        "      # Normalize the data to [0, 1] range\n",
        "      scaler = MinMaxScaler()\n",
        "      scaled_data = scaler.fit_transform(df)\n",
        "      df_scaled = pd.DataFrame(scaled_data, columns=categories, index=df.index)\n",
        "    else:\n",
        "      df_scaled = df\n",
        "    num_rows = len(df)\n",
        "\n",
        "    # Generate a color palette with a unique color for each row\n",
        "    palette = sns.color_palette(\"tab10\", n_colors=num_rows)\n",
        "\n",
        "    if single_chart:\n",
        "        # Create a single radar chart for all data\n",
        "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
        "\n",
        "        # Plot each row data on the same radar chart with different colors\n",
        "        for i, (index, row) in enumerate(df_scaled.iterrows()):\n",
        "            color = palette[i % len(palette)]  # Use color from palette\n",
        "            create_radar_chart(ax, row, categories, color,single_chart, label=index)\n",
        "\n",
        "        # Add the legend\n",
        "        ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
        "\n",
        "        # Set the title for the single radar chart\n",
        "        ax.set_title(custom_title, size=20, color='Black', y=1.1)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        # Calculate the number of rows needed for a 2-column layout\n",
        "        num_cols = 2\n",
        "        num_rows_plot = int(np.ceil(num_rows / num_cols))\n",
        "\n",
        "        # Create a figure with subplots\n",
        "        fig, axs = plt.subplots(num_rows_plot, num_cols, subplot_kw=dict(polar=True), figsize=(15, num_rows_plot * 6))\n",
        "\n",
        "        # Flatten the axes array if necessary\n",
        "        axs = axs.flatten()\n",
        "\n",
        "        # Adjust the layout\n",
        "        plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "        # Generate radar charts for each row\n",
        "        for i, (index, row) in enumerate(df_scaled.iterrows()):\n",
        "            ax = axs[i]\n",
        "            color = palette[i % len(palette)]  # Use color from palette\n",
        "            create_radar_chart(ax, row, categories, color,single_chart, label=index)\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for j in range(i + 1, len(axs)):\n",
        "            axs[j].axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFeAaQXZ1J-5"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZbF9Ung1PKL"
      },
      "source": [
        "# General Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSZ9FXg1rua"
      },
      "source": [
        "### General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfsqSfKhXYFm"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFSAF0UtXUhP"
      },
      "outputs": [],
      "source": [
        "temp_df = df[['online_order','book_table','weighted_rating','votes','num_cuisines','num_menu_item','num_reviews','num_dish_liked','num_spec','is_road','approx_cost(for two people)']]\n",
        "temp_df.replace({'Yes': 1, 'No': 0}, inplace=True)\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(temp_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc5WiOY5fN8P"
      },
      "source": [
        "Among the factors influencing price, the options for booking tables, the number of liked dishes, votes, and ratings are the most variable. This is true when other fixed features, such as location, are set aside."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIBZhq90LxH5"
      },
      "source": [
        "**Which are top restaurants?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cUcgmeNLzrK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "chains=df['name'].value_counts()[:20]\n",
        "sns.barplot(x=chains,y=chains.index,palette='deep')\n",
        "plt.title(\"Most famous restaurants\")\n",
        "plt.xlabel(\"Number of outlets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0otldoKFMfcL"
      },
      "outputs": [],
      "source": [
        "temp_df = df[['name','votes','weighted_rating','approx_cost(for two people)']].groupby('name').median().sort_values('votes',ascending=False)[:20]\n",
        "types_frq = df['name'].value_counts().reset_index().rename(columns={'index':'name','count':'count'})\n",
        "types_frq = types_frq[types_frq['name'].isin(temp_df.index)]\n",
        "multi_bar_chart(temp_df,'Analysis of top 20 restaurants In Votes','Restaurant Names',types_frq=types_frq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxPnRFFpVE-K"
      },
      "outputs": [],
      "source": [
        "# Check types for cafe Coffee Day, Onesta, and Just Bake and Brewski, Toit, and The Black Pearl\n",
        "df[df.name.isin(['Cafe Coffee Day','Onesta','Just Bake','Byg Brewski Brewing Company', 'Toit','The Black Pearl'])][['name','listed_in(type)']].groupby('name')['listed_in(type)'].apply(Counter).dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ArGxjVBdigT"
      },
      "source": [
        "\n",
        "#### Overview\n",
        "\n",
        "In Bangalore, several restaurant and cafe chains have established a strong presence, with notable examples including Caf Coffee Day, Onesta, and Just Bake. Despite their high number of outlets, certain establishments like Byg Brewski Brewing Company, Toit, and The Black Pearl excel in customer engagement and votes.\n",
        "\n",
        "#### Key Insights\n",
        "\n",
        "1. **Popularity and Outlet Distribution:**\n",
        "   - **Caf Coffee Day**: With a substantial number of outlets across various types27 in Delivery, 23 in Dine-out, 27 in Cafes, and 19 in DessertsCaf Coffee Day holds a significant share of the market. Its wide distribution indicates a strong market presence and widespread consumer accessibility.\n",
        "   - **Just Bake**: This chain focuses heavily on Delivery (31 outlets) and Desserts (31 outlets), showcasing its specialization in quick-service and dessert offerings. Its high number of delivery outlets suggests a strong demand for convenience and dessert options in Bangalore.\n",
        "   - **Onesta**: Operating with 24 Delivery, 21 Dine-out, 21 Cafes, 9 Desserts, and 10 Buffet outlets, Onestas diversified approach highlights its strategy to cater to various dining preferences, from quick bites to buffet options.\n",
        "\n",
        "2. **Customer Engagement and Votes:**\n",
        "   - **Byg Brewski Brewing Company**: Despite having fewer outlets (with 2 each in Delivery, Dine-out, and Drinks & Nightlife), Byg Brewski stands out in terms of customer engagement and votes. This suggests that while it operates on a smaller scale, it has successfully created a high-value, engaging experience for its patrons.\n",
        "   - **Toit**: Known for its Dine-out and Drinks & Nightlife options, Toit also has a strong presence in the market. Its limited number of outlets (one each in Dine-out and Drinks & Nightlife) indicates a focused strategy, resulting in significant customer engagement despite fewer locations.\n",
        "   - **The Black Pearl**: This establishments success in Dine-out and Buffet categories, with seven outlets each, and Pubs and Bars (four outlets), demonstrates its ability to attract and engage customers through a niche offering.\n",
        "\n",
        "#### Market Implications\n",
        "\n",
        "1. **Diverse Customer Preferences**: The data suggests a variety of customer preferences in Bangalore. Chains like Caf Coffee Day and Just Bake cater to high-volume, quick-service needs, while Byg Brewski and Toit offer specialized experiences that garner high engagement despite having fewer outlets. This indicates that customer engagement is not solely dependent on the number of outlets but also on the quality and type of experience offered.\n",
        "\n",
        "2. **Strategic Focus and Engagement**: The success of Byg Brewski and Toit highlights the effectiveness of a focused approach. Establishments that concentrate on delivering a unique experiencesuch as Byg Brewskis brewery setting or Toits brewery and dining experiencecan achieve higher engagement and votes, even with a smaller footprint.\n",
        "\n",
        "3. **Opportunities for Growth**: For investors, the findings suggest opportunities in both expanding popular chains and investing in niche concepts that offer high engagement. Chains with a high number of outlets should focus on enhancing customer experience to maintain and boost engagement. Conversely, specialized establishments can explore scaling their operations while preserving their unique appeal.\n",
        "\n",
        "In summary, the Bangalore market shows a dynamic landscape where both high-volume chains and niche establishments play crucial roles. Understanding and leveraging customer preferences and engagement metrics can guide strategic decisions for growth and investment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m7ksBlZQOd4"
      },
      "source": [
        "### List Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlhLNbyzR-lF"
      },
      "outputs": [],
      "source": [
        "pie_chart(df,'listed_in(type)','List Type',figsize=[10,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6rmBaaqeeLN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "temp_df = df[['listed_in(type)','votes','weighted_rating','approx_cost(for two people)']].groupby('listed_in(type)').median().sort_values('votes',ascending=False)\n",
        "types_frq = df['listed_in(type)'].value_counts().reset_index().rename(columns={'index':'name','count':'count'})\n",
        "types_frq = types_frq[types_frq['listed_in(type)'].isin(temp_df.index)]\n",
        "multi_bar_chart(temp_df,'Top general types iterm of Votes','Types Names',types_frq=types_frq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB-pqrDM_fLf"
      },
      "outputs": [],
      "source": [
        "\n",
        "radar_df = temp_df.merge(df['listed_in(type)'].value_counts(),how='left',left_index=True, right_index=True)\n",
        "radar_df = radar_df.rename(columns={'approx_cost(for two people)':'Price','weighted_rating':'Rating','votes':'Engagement','count':'Traffic'})\n",
        "plot_radar_charts(radar_df,single_chart=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmwFN4sujj7c"
      },
      "source": [
        "#### Overview\n",
        "\n",
        "Based on the analysis of restaurant types and their performance in Bangalore, the data reveals notable trends in customer preferences, engagement, and ratings. Here's a summary of the findings and the implications for the market.\n",
        "\n",
        "#### Key Findings\n",
        "\n",
        "1. **Type Distribution**:\n",
        "   - **Delivery**: 50%\n",
        "   - **Dine-out**: 34%\n",
        "   - **Desserts**: 7%\n",
        "\n",
        "2. **Top Performing Categories**:\n",
        "   - **Drinks & Nightlife**: Top category in terms of engagement and ratings.\n",
        "   - **Buffet** and **Pubs and Bars**: Slightly lower in engagement and ratings but still notable.\n",
        "\n",
        "3. **Lower Performing Categories**:\n",
        "   - **Delivery**, **Dine-out**, and **Desserts**: These categories show significantly lower votes and ratings compared to others.\n",
        "\n",
        "#### Market Implications\n",
        "\n",
        "1. **Customer Preferences**:\n",
        "   - **Drinks & Nightlife** establishments excel in terms of engagement and ratings. This suggests a strong consumer preference for venues that offer a vibrant social experience, which often includes entertainment and a lively atmosphere. The higher engagement indicates that customers are willing to invest time and money in these experiences.\n",
        "\n",
        "2. **Delivery, Dine-out, and Desserts**:\n",
        "   - These categories, despite having a high percentage of establishments, show lower engagement and ratings. This could imply that while these options are popular and widely available, they may not offer the unique or high-quality experiences that drive high engagement and satisfaction. The lower performance could be attributed to factors such as service quality, food variety, or overall dining experience.\n",
        "\n",
        "#### Opportunities\n",
        "\n",
        "1. **Enhancing the Drinks & Nightlife Experience**:\n",
        "   - The high engagement and ratings for Drinks & Nightlife venues highlight a significant opportunity for investment in this category. Establishments that offer unique, high-quality experiences in this sector are likely to attract more customers and achieve higher ratings. Investing in creating memorable experiences and focusing on high-quality service can yield substantial returns.\n",
        "\n",
        "2. **Improving Delivery and Dine-out Services**:\n",
        "   - To address the lower engagement and ratings in Delivery, Dine-out, and Desserts, businesses should focus on improving the quality and uniqueness of their offerings. This could involve enhancing the delivery experience, offering exclusive or premium dining options, or innovating in dessert offerings. By addressing customer feedback and focusing on quality, these categories can potentially improve their performance and customer satisfaction.\n",
        "\n",
        "3. **Diversifying Offerings**:\n",
        "   - There is an opportunity to diversify offerings within the high-performing categories. For example, integrating elements of Drinks & Nightlife into Dine-out experiences could create a more appealing and engaging environment, thereby boosting ratings and customer engagement.\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "The analysis indicates a strong consumer preference for Drinks & Nightlife experiences, which presents a lucrative opportunity for investment. Meanwhile, there is potential for improvement in the Delivery, Dine-out, and Desserts categories. By focusing on enhancing the quality and uniqueness of these offerings, businesses can better align with customer preferences and drive higher engagement and satisfaction.\n",
        "\n",
        "Investors should consider these insights to make informed decisions and capitalize on the opportunities within the Bangalore restaurant market.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOaGccpk1W7Q"
      },
      "source": [
        "### Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6DMvHKSyE3M"
      },
      "source": [
        "How Rating Effect Priceing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBcfH_FZzNNc"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srAEuL17whhP"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(19, 8))\n",
        "\n",
        "# Scatter plot for rate vs cost\n",
        "sns.scatterplot(y='approx_cost(for two people)',x='weighted_rating',data=df,hue='book_table',alpha=0.8,palette='Set2',ax=ax[0,0])\n",
        "ax[0,0].set_title('Cost & Booking Table vs Weighted Rating')\n",
        "ax[0,0].set_xlabel('Weighted Rating')\n",
        "ax[0,0].set_ylabel('Cost')\n",
        "\n",
        "# Line plot for rate vs weighted_rating\n",
        "sns.scatterplot(y='approx_cost(for two people)',x='weighted_rating',data=df,hue='online_order',alpha=0.8,palette='Set2',ax=ax[0,1])\n",
        "ax[0,1].set_title('Cost & Online Order vs Weighted Rating')\n",
        "ax[0,1].set_ylabel('Cost')\n",
        "ax[0,1].set_xlabel('Weighted Rating')\n",
        "\n",
        "\n",
        "sns.scatterplot(y='approx_cost(for two people)',x='votes',data=df,alpha=0.5,ax=ax[1,0])\n",
        "ax[1,0].set_title('Cost & Votes vs Votes')\n",
        "ax[1,0].set_xlabel('Votes')\n",
        "ax[1,0].set_ylabel('Cost')\n",
        "\n",
        "# Line plot for rate vs weighted_rating\n",
        "sns.scatterplot(y='approx_cost(for two people)',x='weighted_rating',data=df,hue='is_road',alpha=0.5,palette='Set2',ax=ax[1,1])\n",
        "ax[1,1].set_title('Cost & On Road vs Weighted Rating')\n",
        "ax[1,1].set_ylabel('Cost')\n",
        "ax[1,1].set_xlabel('Weighted Rating')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNbdG3E5AJOl"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(19, 8))\n",
        "\n",
        "# Scatter plot for rate vs cost\n",
        "sns.boxplot(y='approx_cost(for two people)',x='book_table',data=df,ax=ax[0,0])\n",
        "ax[0,0].set_title('Cost vs Booking Table')\n",
        "ax[0,0].set_xlabel('Booking Table')\n",
        "ax[0,0].set_ylabel('Cost')\n",
        "\n",
        "# Line plot for rate vs weighted_rating\n",
        "sns.boxplot(y='approx_cost(for two people)',x='online_order',data=df,ax=ax[0,1])\n",
        "ax[0,1].set_title('Cost vs Online Order')\n",
        "ax[0,1].set_ylabel('Cost')\n",
        "ax[0,1].set_xlabel('Online Order')\n",
        "\n",
        "\n",
        "sns.boxplot(y='approx_cost(for two people)',x='is_road',data=df,ax=ax[1,0])\n",
        "ax[1,0].set_title('Cost vs Road')\n",
        "ax[1,0].set_xlabel('Votes')\n",
        "ax[1,0].set_ylabel('Road')\n",
        "\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHltqcCS_4A1"
      },
      "source": [
        "## Market Analysis: The Impact of Booking Tables, Online Orders, and Location on Restaurant Pricing\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The Indian restaurant and cafe market is characterized by diverse consumer preferences and business models. This analysis examines the influence of booking tables, online ordering, location, and customer feedback (ratings and votes) on pricing strategies within this sector.\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "1. **Booking Tables and Pricing**\n",
        "\n",
        "   Booking tables at restaurants show a strong correlation with pricing. Establishments that offer table reservations tend to have higher price points. This is likely due to the premium experience associated with dine-in services, which includes not only the food but also the ambiance and personalized service. Consumers are often willing to pay more for the assurance of a reserved spot, especially in popular or high-demand venues.\n",
        "\n",
        "2. **Online Orders and High-Cost Restaurants**\n",
        "\n",
        "   High-cost restaurants often do not offer online ordering services. This is primarily because the experience of dining in such establishments includes being physically present to enjoy the environment and service, which cannot be replicated through home delivery. Additionally, the logistical challenges and potential compromise on food quality during delivery deter high-end restaurants from offering online orders.\n",
        "\n",
        "3. **Impact of Location**\n",
        "\n",
        "   Restaurants located on main roads better than within residential areas exhibit a significant effect on pricing strategies. Being in a prime location allows these establishments to command higher prices due to increased visibility and accessibility. Moreover, such locations often attract a broader customer base, enabling them to cover a wider price range to cater to diverse economic segments.\n",
        "\n",
        "4. **Votes and Ratings Influence**\n",
        "\n",
        "   While customer votes (the number of reviews) have a limited impact on pricing, ratings (the quality of reviews) significantly influence price levels. An increase in ratings from 3.5 to 4.3 is typically associated with higher prices, as it reflects consumer satisfaction and perceived value. However, beyond a rating of 4.3, prices tend to decrease. This trend suggests that to achieve exceptionally high ratings, restaurants might lower prices to enhance value perception and attract more customers, creating a balance between cost and quality.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The dynamics of the Indian restaurant market reveal that consumer preferences and business strategies are intricately linked. High-rated establishments often find themselves adjusting pricing to maintain quality and customer satisfaction. For investors, understanding these nuances can guide strategic decisions in the food service industry, emphasizing the importance of location, service offerings, and consumer engagement in determining pricing strategies.\n",
        "\n",
        "### Recommendations for Investors\n",
        "\n",
        "- **Focus on Location**: Invest in restaurants with strategic locations that naturally attract more foot traffic and can justify higher pricing.\n",
        "\n",
        "- **Enhance Customer Experience**: Encourage businesses to offer table bookings to capitalize on consumers' willingness to pay for a guaranteed dining experience.\n",
        "\n",
        "- **Balance Pricing and Quality**: For high-rating targets, focus on maintaining quality and adjusting prices to stay competitive without compromising the customer experience.\n",
        "\n",
        "- **Leverage Customer Feedback**: Use ratings and reviews as critical data points for continuous improvement and strategic pricing adjustments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d7Fuijk1ZPp"
      },
      "source": [
        "### Location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWL0vagpavUZ"
      },
      "source": [
        "Which is the foodi area?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kSDJa4faurq"
      },
      "outputs": [],
      "source": [
        "# Calculate the value counts and reset the index\n",
        "temp_df = df['location'].value_counts().reset_index()\n",
        "temp_df.columns = ['location', 'count']\n",
        "traffic_df = temp_df.copy()\n",
        "count_chart_with_percentage(temp_df,'location','Top 30 Foodie Areas','Locations',[0,30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Vh41uTIQ7j"
      },
      "source": [
        "What is the effect of location on the on other variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUwzQJYUIMO_"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "temp_df = df[['location','votes','weighted_rating','approx_cost(for two people)']].groupby('location').median().sort_values('approx_cost(for two people)',ascending=False)[:40]\n",
        "types_frq = df['location'].value_counts().reset_index().rename(columns={'index':'name','count':'count'})\n",
        "types_frq = types_frq[types_frq['location'].isin(temp_df.index)]\n",
        "radar_df = temp_df.head(3)\n",
        "multi_bar_chart(temp_df,'Location vs Cost','Locations',types_frq=types_frq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asjPWguMdO9t"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "temp_df = df[['location','votes','weighted_rating','approx_cost(for two people)']].groupby('location').median().sort_values('votes',ascending=False)[:40]\n",
        "types_frq = df['location'].value_counts().reset_index().rename(columns={'index':'name','count':'count'})\n",
        "types_frq = types_frq[types_frq['location'].isin(temp_df.index)]\n",
        "radar_df = pd.concat([radar_df,temp_df.head(3)])\n",
        "multi_bar_chart(temp_df,'Location vs Votes','Locations',types_frq=types_frq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVcXgzJLOlkm"
      },
      "outputs": [],
      "source": [
        "\n",
        "size_map = {\n",
        "    'Buffet': 300,\n",
        "    'Cafes': 200,\n",
        "    'Pubs & Bars': 100,\n",
        "    'Delivery': 80,\n",
        "    'Desserts': 60,\n",
        "    'Dine-out': 50,\n",
        "    'Drinks & nightlife': 50\n",
        "}\n",
        "temp_df = df.copy(deep=True)\n",
        "temp_df['size'] = temp_df['listed_in(type)'].map(size_map)\n",
        "plt.figure(figsize=(15,10))\n",
        "palette = sns.color_palette('rainbow', n_colors=len(temp_df['listed_in(type)'].unique()))\n",
        "sns.scatterplot(\n",
        "    x='lon', y='lat',\n",
        "    data=temp_df,\n",
        "    hue='listed_in(type)',\n",
        "    style='listed_in(type)',\n",
        "    palette=palette,\n",
        "    markers=['o', 's', 'D', 'X', 'v', '^', '<', '>'],\n",
        "    s=300,\n",
        "    size='size',\n",
        "    sizes=(100, 300)\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Scatter Plot of Locations')\n",
        "plt.legend(title='Listed In Type')\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5-7xRiaWIVz"
      },
      "outputs": [],
      "source": [
        "df['listed_in(type)'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q9MKXNlMqId"
      },
      "source": [
        "Spatial Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnc_fpzsMpkx"
      },
      "outputs": [],
      "source": [
        "from folium.plugins import FastMarkerCluster\n",
        "\n",
        "def generate_map(latitude=12.911276\t,longitude=77.604565,zoom_start=10):\n",
        "  m = folium.Map(location=[latitude, longitude], zoom_start=zoom_start)\n",
        "  return m\n",
        "\n",
        "basemap = generate_map()\n",
        "HeatMap(data=df[['lat','lon']],zoom=20).add_to(basemap)\n",
        "# Create HTML for the vertical gradient bar legend\n",
        "legend_html = '''\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 50px; width: 40px; height: 250px;\n",
        "    background: linear-gradient(to top, blue, green, yellow, red);\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0px 0px 12px rgba(0, 0, 0, 0.2);\n",
        "    padding: 5px;\n",
        "    font-size: 12px;\n",
        "    font-family: Arial, sans-serif;\n",
        "    z-index: 1000;\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    justify-content: space-between;\n",
        "\">\n",
        "    <div style=\"text-align: center; font-weight: bold;\">Intensity</div>\n",
        "</div>\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 95px; height: 250px;\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    justify-content: space-between;\n",
        "    font-size: 12px;\n",
        "    font-family: Arial, sans-serif;\n",
        "    z-index: 1000;\n",
        "    font-weight: bold;\n",
        "\">\n",
        "    <div>High</div>\n",
        "    <div>Miduam</div>\n",
        "    <div>Low</div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# Add legend to the map\n",
        "basemap.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "# Display the map\n",
        "basemap.save('map_with_vertical_gradient_legend_bold_labels.html')\n",
        "\n",
        "# Add marker clustring\n",
        "FastMarkerCluster(data=df[['lat','lon']],zoom=20).add_to(basemap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5MuUnqJOHJx"
      },
      "outputs": [],
      "source": [
        "basemap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G-17B0b1q0Z"
      },
      "outputs": [],
      "source": [
        "temp_df = df[['location','votes','weighted_rating','approx_cost(for two people)']].groupby('location').median().sort_values('votes',ascending=False)\n",
        "temp_df = temp_df[temp_df.index.isin(traffic_df.head(3).location.tolist())]\n",
        "radar_df = pd.concat([radar_df,temp_df])\n",
        "radar_df = radar_df.merge(df['location'].value_counts(),how='left',left_index=True, right_index=True)\n",
        "radar_df = radar_df.rename(columns={'approx_cost(for two people)':'Price','weighted_rating':'Rating','votes':'Engagement','count':'Traffic'})\n",
        "radar_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGAA9q-L6Oj5"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(radar_df,single_chart=True,custom_title='Locations Characteristics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMQHOntwGR67"
      },
      "source": [
        "Bangalore's dynamic culinary landscape offers various opportunities for strategic investment. By examining the distribution and characteristics of dining establishments across key neighborhoods, investors can make informed decisions.\n",
        "\n",
        "## Centralized Nightlife Venues\n",
        "- **Drinks & Nightlife**: Concentrated in the heart of Bangalore, these establishments cater to the city's vibrant, tech-savvy young professionals and expats seeking entertainment and social experiences. The central locations offer high visibility and access to a diverse clientele. Investment in these areas should focus on innovative concepts that combine local culture with international trends to attract a wide audience.\n",
        "\n",
        "## Western Buffet Offerings\n",
        "- **Buffet**: Predominantly located on the western side of the city center, buffets appeal to families and groups. These venues should emphasize diverse culinary options and value for money to attract the surrounding residential communities. Expanding in these areas can capitalize on the demand for family-friendly dining experiences.\n",
        "\n",
        "## Emerging Restaurant Hubs\n",
        "- **Whitefield, Electronic City, BTM Layout, HSR Layout, Marathahalli**: These neighborhoods account for nearly 30% of the city's restaurants, with Whitefield alone comprising 10%. Known for their vibrant youth culture and burgeoning tech industry, these areas are ideal for casual dining and quick-service restaurants. Investors should focus on creating hip, affordable venues that cater to students, young professionals, and tech workers.\n",
        "\n",
        "## High-Spending Customer Zones\n",
        "- **Sankey Road, Lavelle Road, Race Course Road, Infantary Road**: These affluent areas attract customers with higher spending power, making them suitable for upscale dining establishments. Restaurants here should offer gourmet cuisine, exceptional service, and a premium ambiance to meet the expectations of discerning diners. Innovative and exclusive dining concepts will thrive in these high-value zones.\n",
        "\n",
        "## Engagement-Driven Destinations\n",
        "- **Rajarajeshwari nagar, Lavelle Road, Church Street**: Known for high engagement and vibrant atmospheres, these areas attract patrons seeking unique culinary experiences. Establishments should focus on creating interactive and memorable dining experiences, such as themed decor, live performances, or fusion menus that highlight both global and local flavors.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Bangalore's diverse neighborhoods offer varied opportunities for restaurant investments. By aligning restaurant concepts with the unique characteristics and customer profiles of each area, investors can optimize market reach and profitability. Understanding local consumer behavior, leveraging the city's tech-driven innovation, and maintaining cultural relevance will be key to successful ventures in this bustling metropolis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFtUvNpB14as"
      },
      "source": [
        "## Resturants Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzfu0btMnOVB"
      },
      "source": [
        "**What are the different specializations in the food sector, and what are their characteristics?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHykugmTmqmk"
      },
      "outputs": [],
      "source": [
        "# Assuming df is already defined with columns 'rest_type_1' and 'rest_type_2'\n",
        "rest_types = df['rest_type_0'].tolist() + df['rest_type_1'].tolist()\n",
        "types_df = pd.DataFrame(rest_types, columns=['rest_type'])\n",
        "types_df = types_df[types_df.rest_type != '-']\n",
        "types_frq = types_df.value_counts().reset_index()\n",
        "types_frq.columns = ['rest_type', 'count']\n",
        "count_chart_with_percentage(types_frq,'rest_type','Most Common Restaurant Type','Restaurant Type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbL8ZO2UncRk"
      },
      "outputs": [],
      "source": [
        "pie_chart(types_df[types_df.rest_type.isin(types_frq.rest_type.head(10).tolist())],'rest_type','Rest Type',figsize=[10,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cbj4-wSgH3H"
      },
      "outputs": [],
      "source": [
        "temp_df=df[['rest_type_0','approx_cost(for two people)','weighted_rating','votes','location']].rename(columns={'rest_type_0':'rest_type'})\n",
        "temp_df= pd.concat([temp_df,df[['rest_type_1','approx_cost(for two people)','weighted_rating','votes','location']][df.rest_type_1 != '-'].rename(columns={'rest_type_1':'rest_type'})])\n",
        "count_types = temp_df.drop('location',axis=1).groupby('rest_type').median().sort_values(by='approx_cost(for two people)').copy()\n",
        "freq_types = temp_df['rest_type'].value_counts().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwJyvJ3vk1Cm"
      },
      "outputs": [],
      "source": [
        "multi_bar_chart(count_types,'Approximate Cost and Weighted Rating by Restaurant Type','Restaurant Types',types_frq=freq_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I43ocWGaozmC"
      },
      "outputs": [],
      "source": [
        "radar_df = count_types.merge(freq_types,how='left',left_on='rest_type', right_on='rest_type')\n",
        "radar_df = radar_df.rename(columns={'approx_cost(for two people)':'Price','weighted_rating':'Rating','votes':'Engagement','count':'Traffic'})\n",
        "radar_df = radar_df.sort_values('Price',ascending=False)\n",
        "radar_df = pd.concat([radar_df.head(9),radar_df[radar_df.rest_type=='Quick Bites']])\n",
        "radar_df = radar_df.set_index('rest_type')\n",
        "radar_df.drop_duplicates(inplace=True)\n",
        "plot_radar_charts(radar_df,single_chart=True,custom_title='specializations  Characteristics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEeKRW4UEhxa"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(radar_df,single_chart=False,custom_title='specializations  Characteristics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3sdAeByeK1"
      },
      "source": [
        "\n",
        "## Market Overview\n",
        "\n",
        "Bangalore's food scene is vibrant and diverse, encompassing a wide range of dining options. The market is primarily divided into three main categories:\n",
        "\n",
        "1. **Quick Bites (40%)**: This category includes fast-food outlets and quick-service restaurants. While it constitutes a significant portion of the market, it lacks strong customer engagement.\n",
        "\n",
        "2. **Casual Dining and Cafes**: These segments together account for about two-thirds of the food market. Casual Dining and Cafes have higher customer engagement, suggesting that diners prefer a mix of convenience and a pleasant dining atmosphere.\n",
        "\n",
        "3. **Irani Cafes**: These are currently trending due to their engaging atmosphere, reasonable pricing, and high ratings (up to 4.4). Irani Cafes offer unique dining experiences, filling a niche in the market with limited competition.\n",
        "\n",
        "## Investment Opportunities\n",
        "\n",
        "1. **Fine Dining**: Although this sector is the most expensive and currently has limited customer interest, it presents an opportunity for experienced investors to develop exceptional dining experiences for high-end customers.\n",
        "\n",
        "2. **Drinks and Nightlife**: Pubs, microbreweries, and clubs are popular among younger demographics and show strong demand. Investing in these venues can be lucrative due to their popularity and relatively good pricing.\n",
        "\n",
        "## Customer Types and Best Locations\n",
        "\n",
        "1. **Young Professionals and Millennials**: This group favors microbreweries, pubs, and clubs. Ideal locations for these venues are busy areas with vibrant nightlife.\n",
        "\n",
        "2. **Families and Casual Diners**: Families prefer Casual Dining and Cafes, which are best situated in suburban areas with a community-oriented vibe.\n",
        "\n",
        "3. **Wealthy and Special Occasion Diners**: Fine Dining establishments cater to high-income individuals and special events. These should be located in upscale neighborhoods or near cultural landmarks.\n",
        "\n",
        "4. **Culture Lovers**: Irani Cafes appeal to those interested in traditional and cultural dining experiences. These cafes perform well in historical areas that complement their cultural theme.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Bangalores food market presents diverse opportunities for investors. Irani Cafes stand out as a promising investment due to their unique appeal and limited competition. By aligning investment strategies with customer preferences and selecting appropriate locations, investors can effectively tap into the citys varied dining needs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sULcWEVB19xD"
      },
      "source": [
        "### Location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g6nzYR91Yi7"
      },
      "source": [
        "**What is the most known location for each resturant type?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-CSYuZTLV7y"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import to_hex\n",
        "temp_df = temp_df.rename(columns={'votes': 'Count'})\n",
        "# Group the DataFrame\n",
        "grouped_df = (\n",
        "    temp_df[['location', 'rest_type', 'Count']]\n",
        "    .groupby(['location', 'rest_type'])\n",
        "    .count()\n",
        "    .reset_index()\n",
        "    .groupby('rest_type')\n",
        "    .max()\n",
        "    .sort_values(['location', 'Count'], ascending=False)\n",
        "\n",
        ")\n",
        "def styling_df(grouped_df,col='location'):\n",
        "  # Get unique locations and assign a unique color to each\n",
        "  unique_locations = grouped_df[col].unique()\n",
        "  colors = sns.color_palette(\"pastel\", len(unique_locations))  # Generate unique colors\n",
        "  location_colors = dict(zip(unique_locations, colors))  # Map each location to a color\n",
        "\n",
        "  # Define a function to apply styling to the 'location' column\n",
        "  def highlight_locations(s):\n",
        "      return [f'background-color: {to_hex(location_colors[val])}' for val in s]\n",
        "\n",
        "  # Style the DataFrame\n",
        "  if 'Price' in grouped_df.columns:\n",
        "\n",
        "    styled_df = (\n",
        "        grouped_df.style\n",
        "        .background_gradient(subset='Count', cmap='YlGnBu')\n",
        "        .background_gradient(subset='Price', cmap='YlGnBu')\n",
        "        .background_gradient(subset='Rating', cmap='YlGnBu')\n",
        "        .apply(highlight_locations, subset=[col])\n",
        "    )\n",
        "  else:\n",
        "    styled_df = (\n",
        "        grouped_df.style\n",
        "        .background_gradient(subset='Count', cmap='YlGnBu')\n",
        "        .apply(highlight_locations, subset=[col])\n",
        "    )\n",
        "\n",
        "  # Display the styled DataFrame\n",
        "  return styled_df\n",
        "\n",
        "styled_df = styling_df(grouped_df)\n",
        "styled_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gasetfT613-0"
      },
      "source": [
        "### Location and Socioeconomic analysis\n",
        "\n",
        "**1. Yeshwantpur**  \n",
        "Yeshwantpur stands out for its wide range of affordable food options, showcasing significant diversity in restaurant types. This diversity, combined with budget-friendly pricing, appeals primarily to the middle class, who are looking for varied food experiences without exceeding average spending limits. The variety here supports a broad spectrum of tastes and preferences, making it a popular choice for those with moderate budgets.\n",
        "\n",
        "**2. Wilson Garden**  \n",
        "In contrast, Wilson Garden has a more limited range of food options. Its primary appeal lies in its affordability and focus on fast food. This characteristic makes it attractive to budget-conscious diners seeking quick, economical meals. The limited diversity in food types reflects its niche focus on low-budget, fast-food offerings.\n",
        "\n",
        "**3. Whitefield**  \n",
        "Whitefield offers a diverse range of food options catering to mid-to-high budget ranges. This neighborhood serves as a midpoint between middle-class and high-class food experiences. The areas offerings appeal to both upper-middle-class and lower-high-class patrons. Additionally, Whitefields vibrant nightlife contributes to its attractiveness, drawing in those seeking both quality food and evening entertainment.\n",
        "\n",
        "**4. West Bangalore**  \n",
        "Known for its food trucks, West Bangalore caters to a wide demographic, including people of various ages, classes, and backgrounds. The specialization in food trucks provides a unique, casual food experience that appeals to a large and diverse audience. This format supports the community's varied preferences and enhances accessibility.\n",
        "\n",
        "**5. Sankey Road and Lavelle Road**  \n",
        "Both Sankey Road and Lavelle Road are renowned for their exclusive food experiences. These areas often feature unique or high-end restaurants that are sometimes the only outlets of their kind in the city. This exclusivity attracts high-class and high-ticket customers who are looking for premium food experiences. Their reputation for offering one-of-a-kind food options further solidifies their appeal to affluent patrons.\n",
        "\n",
        "**6. Kalyan Nagar**  \n",
        "Kalyan Nagar predominantly attracts upper-middle-class residents. The area provides a balanced mix of food options that cater to this demographics preferences and budget. It offers a comfortable and accessible food experience that aligns with the lifestyle of upper-middle-class individuals.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Each neighborhood in Bangalore has its unique characteristics that cater to different segments of the food market:\n",
        "\n",
        "- **Yeshwantpur**: Diverse and affordable, appealing to the middle class.\n",
        "- **Wilson Garden**: Budget-friendly and fast food, suited for economical diners.\n",
        "- **Whitefield**: Mid-to-high budget, attracting both upper-middle-class and lower-high-class individuals, with a focus on nightlife.\n",
        "- **West Bangalore**: Food trucks offering diverse options, drawing a broad demographic.\n",
        "- **Sankey Road and Lavelle Road**: Exclusive food experiences for high-class and high-ticket customers.\n",
        "- **Kalyan Nagar**: Upper-middle-class neighborhood with a balanced food scene.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs0_cc592mtF"
      },
      "source": [
        "#### Dishes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWrGE9YbgyGm"
      },
      "outputs": [],
      "source": [
        "def get_top_dish(df,num=3):\n",
        "  combined_list = [item for sublist in df['dish_liked'] for item in sublist if item != '-']\n",
        "  count_unique = Counter(combined_list)\n",
        "  sorted_count_unique = dict(count_unique.most_common(num))\n",
        "  return sorted_count_unique\n",
        "\n",
        "temp_df=df[['rest_type_0','approx_cost(for two people)','weighted_rating','votes','location','dish_liked']].rename(columns={'rest_type_0':'rest_type'})\n",
        "temp_df= pd.concat([temp_df,df[['rest_type_1','approx_cost(for two people)','weighted_rating','votes','location','dish_liked']][df.rest_type_1 != '-'].rename(columns={'rest_type_1':'rest_type'})])\n",
        "rest_type_list = styled_df.index.tolist()\n",
        "spec_df = pd.DataFrame(columns=['rest_type','1st', '2nd', '3rd'])\n",
        "for rest in rest_type_list:\n",
        "  output = get_top_dish(temp_df[temp_df.rest_type==rest])\n",
        "  t_df = pd.DataFrame(columns=['rest_type','1st', '2nd', '3rd'])\n",
        "  if len(output)==0:\n",
        "    continue\n",
        "  t_df.loc[0,'1st'] = list(output.keys())[0]\n",
        "  t_df.loc[0,'2nd'] = list(output.keys())[1]\n",
        "  if len(output)>2:\n",
        "    t_df.loc[0,'3rd'] = list(output.keys())[2]\n",
        "  else:\n",
        "    t_df.loc[0,'3rd'] = '-'\n",
        "  t_df.loc[0,'rest_type'] = rest\n",
        "  spec_df = pd.concat([spec_df,t_df])\n",
        "spec_df = spec_df.set_index('rest_type')\n",
        "print('Most preferred Dishes For each type')\n",
        "spec_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s47ifxIv2LDk"
      },
      "source": [
        "## Cuisines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXnCOypByPbD"
      },
      "outputs": [],
      "source": [
        "cols = ['cuisines_0','votes','approx_cost(for two people)','weighted_rating','location']\n",
        "temp_df = df[cols].copy(deep=True)\n",
        "temp_df.rename(columns={'cuisines_0':'cuisines'},inplace=True)\n",
        "for i in range(1,8):\n",
        "  cols = [f'cuisines_{i}','votes','approx_cost(for two people)','weighted_rating','location']\n",
        "  temp_df = pd.concat([temp_df,df[cols].rename(columns={f'cuisines_{i}':'cuisines'})])\n",
        "temp_df = temp_df[temp_df.cuisines != '-']\n",
        "count_values = temp_df.cuisines.value_counts().reset_index()\n",
        "count_values.columns = ['cuisines','count']\n",
        "\n",
        "count_chart_with_percentage(count_values,'cuisines','Most common cuisines','Cuisines',[0,30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDVZCMGk0JOg"
      },
      "outputs": [],
      "source": [
        "multi_bar_chart(temp_df.drop('location',axis=1).groupby('cuisines').median().sort_values(by='approx_cost(for two people)'),'Approximate Cost and Weighted Rating by Cuisines','Cuisines Type',figuresize=(25,10),types_frq=count_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oWYed09B8rI"
      },
      "outputs": [],
      "source": [
        "multi_bar_chart(temp_df.drop('location',axis=1).groupby('cuisines').median().sort_values(by='votes'),'Approximate Cost and Weighted Rating by Cuisines','Cuisines Type',figuresize=(25,10),types_frq=count_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3zXbfRhAOHH"
      },
      "outputs": [],
      "source": [
        "radar_df = temp_df.drop('location',axis=1).groupby('cuisines').median().sort_values(by='votes').merge(count_values.drop(['cumsum','cumperc'],axis=1),how='left',left_index=True, right_on='cuisines')\n",
        "radar_df = radar_df.rename(columns={'approx_cost(for two people)':'Price','weighted_rating':'Rating','votes':'Engagement','count':'Traffic'})\n",
        "radar_df = radar_df.sort_values('Price',ascending=False)\n",
        "radar_df = pd.concat([radar_df.head(7),radar_df[radar_df.cuisines.isin(['North Indian','Chinese','South Indian','African','Singaporean'])]])\n",
        "radar_df = radar_df.set_index('cuisines')\n",
        "radar_df.drop_duplicates(inplace=True)\n",
        "plot_radar_charts(radar_df,single_chart=True,custom_title='specializations  Characteristics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A57PO6FAE7A"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(radar_df,single_chart=False,custom_title='specializations  Characteristics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x1AMOtrK20m"
      },
      "source": [
        "# Investment Analysis Report: Foreign and Local Cuisines in Bangalore\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This report examines the market dynamics of various cuisines in Bangalore, focusing on engagement levels, pricing strategies, and investment potential. Bangalore's cosmopolitan nature creates a diverse culinary landscape, offering opportunities for both local and foreign cuisines to thrive.\n",
        "\n",
        "## Cantonese Cuisine\n",
        "\n",
        "**Analysis:**\n",
        "- **Engagement:** High\n",
        "- **Pricing:** Premium\n",
        "- **Target Audience:** Affluent individuals seeking exclusive dining experiences.\n",
        "- **Profitability:** Significant due to high pricing, but the customer base is niche.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Invest in Cantonese cuisine by emphasizing targeted marketing and exclusive dining experiences. The high price point limits the customer base but ensures high returns per customer.\n",
        "- **Explanation:** High engagement despite premium pricing indicates strong demand among affluent consumers who value authentic experiences. This niche market can be highly profitable but requires targeted strategies to attract and retain customers.\n",
        "\n",
        "## German Cuisine\n",
        "\n",
        "**Analysis:**\n",
        "- **Engagement:** High\n",
        "- **Spreading:** Low\n",
        "- **Pricing:** Lower than Cantonese\n",
        "- **Target Audience:** Middle-income groups looking for authentic yet affordable experiences.\n",
        "- **Profitability:** Balanced between exclusivity and mass appeal.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Focus on providing authentic experiences at competitive prices to attract a broad demographic.\n",
        "- **Explanation:** German cuisines lower price point and moderate engagement suggest it is accessible to a larger audience compared to high-end options. This balance can attract middle-income groups while maintaining profitability.\n",
        "\n",
        "## Sri Lankan, Parsi, and Russian Cuisines\n",
        "\n",
        "**Analysis:**\n",
        "- **Engagement:** High\n",
        "- **Spreading:** Low\n",
        "- **Pricing:** Medium\n",
        "- **Target Audience:** Diners interested in cultural diversity and unique flavors.\n",
        "- **Profitability:** Steady returns with a focus on authenticity and distinctive experiences.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Emphasize cultural authenticity and unique offerings to maintain high engagement.\n",
        "- **Explanation:** Medium pricing combined with high engagement indicates a strong interest in diverse culinary experiences. By highlighting authenticity and unique flavors, these cuisines can sustain their appeal and provide steady returns.\n",
        "\n",
        "## Singaporean Cuisine\n",
        "\n",
        "**Analysis:**\n",
        "- **Engagement:** Growing\n",
        "- **Spreading:** Low\n",
        "- **Pricing:** Moderate\n",
        "- **Target Audience:** Indian audiences interested in diverse culinary experiences.\n",
        "- **Profitability:** Promising, with increasing appeal due to unique flavor profiles and fusion influences.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Leverage strategic marketing, including culinary festivals and pop-up events, to enhance visibility and engagement.\n",
        "- **Explanation:** Singaporean cuisine's emerging popularity aligns with growing interest in diverse food options. Strategic marketing and events can capitalize on this trend and boost engagement.\n",
        "\n",
        "**Supporting Insights:**\n",
        "- [Popularising Singaporean Cuisine Among Indian Audiences](https://bwhotelier.com/article/popularising-singaporean-cuisine-among-indian-audiences-445126)\n",
        "\n",
        "## Foreign vs. Local Cuisines\n",
        "\n",
        "**Analysis:**\n",
        "- **Foreign Cuisines:** Generally attract high engagement and can command premium pricing. There is strong market openness to international flavors.\n",
        "- **Local Cuisines:** Despite comprising 30% of the restaurant market, face saturation and reduced engagement. Consumers seek novelty.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** For local cuisines, focus on innovative approaches such as new regional specialties or fusion dishes.\n",
        "- **Explanation:** The saturation of local cuisines like Northern and Southern Indian reduces consumer engagement. Introducing novel options can rejuvenate interest and offer a competitive edge.\n",
        "\n",
        "## Chinese Cuisine\n",
        "\n",
        "**Analysis:**\n",
        "- **Engagement:** Low\n",
        "- **Spreading:** High\n",
        "- **Pricing:** Variable, often affordable\n",
        "- **Target Audience:** Wide-ranging, with a taste for fusion flavors.\n",
        "- **Popularity:** Ranks second to Northern Indian cuisine.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Continue investing in Chinese cuisine by leveraging its established popularity and introducing innovative dishes.\n",
        "- **Explanation:** The strong market presence and adaptability of Chinese cuisine, coupled with its affordability, contribute to its sustained popularity. Innovative offerings can further enhance its market position.\n",
        "\n",
        "**Supporting Insights:**\n",
        "- [The Rise of Chinese Cuisine in India](https://hongskitchen.in/rise-of-chinese-cuisine-in-india)\n",
        "\n",
        "## African Cuisine\n",
        "\n",
        "**Analysis:**\n",
        "- **Engagement:** Low but with potential for growth\n",
        "- **Spreading:** Low\n",
        "- **Pricing:** Variable\n",
        "- **Target Audience:** Health-conscious and adventurous diners.\n",
        "- **Profitability:** High potential due to low competition; aligning with current health trends.\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Develop a robust marketing strategy focusing on cultural festivals and events to increase engagement and build a loyal customer base.\n",
        "- **Explanation:** Despite currently low engagement, African cuisine's rich flavors and health-oriented offerings align with consumer trends towards diverse and healthy eating. Effective marketing can tap into this potential.\n",
        "\n",
        "**Supporting Insights:**\n",
        "- [India, Africa Surpass China in Reshaping Food Landscape](https://www.foodbusinessnews.net/articles/25632-india-africa-surpass-china-in-reshaping-food-landscape)\n",
        "- [The Rise of African Food](https://www.tenderstem.co.uk/news-events/news/the-rise-of-african-food)\n",
        "\n",
        "## Local Cuisines: Northern and Southern Indian\n",
        "\n",
        "**Analysis:**\n",
        "- **Market Share:** 30% of restaurants\n",
        "- **Competition:** High\n",
        "- **Engagement:** Reduced due to saturation\n",
        "\n",
        "**Recommendation:**\n",
        "- **Investment Strategy:** Innovate within local cuisines by introducing new regional specialties or fusion dishes.\n",
        "- **Explanation:** The high level of competition and saturation in local cuisines necessitates differentiation. Innovation is key to capturing consumer interest and staying relevant in the market.\n",
        "\n",
        "## Journal Article Insights\n",
        "\n",
        "**Study:**\n",
        "- **Title:** \"Restaurants in Little India, Singapore: A Study of Spatial Organization and Pragmatic Cultural Change\"\n",
        "- **Findings:** Offers insights into how restaurants adapt to cultural changes and spatial organization.\n",
        "\n",
        "**Application:**\n",
        "- **Strategy:** Apply insights to organize and position restaurants in Bangalore effectively. Understanding spatial and cultural adaptations will enhance the effectiveness of foreign cuisine offerings.\n",
        "- **Explanation:** Adapting restaurant setups based on cultural and spatial insights can improve market positioning and customer appeal.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Bangalore's diverse food scene presents substantial investment opportunities in both foreign and local cuisines. While foreign cuisines like Cantonese, German, Singaporean, and African offer promising prospects due to their unique appeal and engagement, local cuisines require innovative approaches to capture consumer interest in a saturated market. Strategic investments in marketing and unique culinary experiences are crucial for success.\n",
        "\n",
        "This report incorporates information on Singaporean cuisine and insights from relevant studies, providing a comprehensive overview of the culinary landscape in Bangalore.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNYas4Nz2f2j"
      },
      "source": [
        "### Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7UHNQ_JdaJH"
      },
      "outputs": [],
      "source": [
        "temp_df = temp_df.rename(columns={'votes': 'Count'})\n",
        "# Group the DataFrame\n",
        "grouped_df = (\n",
        "    temp_df[['location', 'cuisines', 'Count']]\n",
        "    .groupby(['location', 'cuisines'])\n",
        "    .count()\n",
        "    .reset_index()\n",
        "    .groupby('cuisines')\n",
        "    .max()\n",
        "    .sort_values(['location', 'Count'], ascending=False)\n",
        "    .merge(temp_df[['location','approx_cost(for two people)']].groupby('location').median().rename(columns={'approx_cost(for two people)':'Price'}),how='left',left_on='location', right_index=True)\n",
        "    .merge(temp_df[['location','weighted_rating']].groupby('location').median().rename(columns={'weighted_rating':'Rating'}),how='left',left_on='location', right_index=True)\n",
        "\n",
        ")\n",
        "\n",
        "styled_df = styling_df(grouped_df)\n",
        "styled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_OrwsW_7CFW"
      },
      "outputs": [],
      "source": [
        "cols = ['cuisines_0','votes','approx_cost(for two people)','weighted_rating','location','dish_liked']\n",
        "temp_df = df[cols].copy(deep=True)\n",
        "temp_df.rename(columns={'cuisines_0':'cuisines'},inplace=True)\n",
        "for i in range(1,8):\n",
        "  cols = [f'cuisines_{i}','votes','approx_cost(for two people)','weighted_rating','location','dish_liked']\n",
        "  temp_df = pd.concat([temp_df,df[cols].rename(columns={f'cuisines_{i}':'cuisines'})])\n",
        "cuisines_list = styled_df.index.tolist()\n",
        "cuisine_spec_df = pd.DataFrame(columns=['cuisines','1st', '2nd', '3rd'])\n",
        "for rest in cuisines_list:\n",
        "\n",
        "  output = get_top_dish(temp_df[temp_df.cuisines==rest])\n",
        "  t_df = pd.DataFrame(columns=['cuisines','1st', '2nd', '3rd'])\n",
        "  if len(output)==0:\n",
        "    continue\n",
        "  t_df.loc[0,'1st'] = list(output.keys())[0]\n",
        "  t_df.loc[0,'2nd'] = list(output.keys())[1]\n",
        "  if len(output)>2:\n",
        "    t_df.loc[0,'3rd'] = list(output.keys())[2]\n",
        "  else:\n",
        "    t_df.loc[0,'3rd'] = '-'\n",
        "  t_df.loc[0,'cuisines'] = rest\n",
        "  cuisine_spec_df = pd.concat([cuisine_spec_df,t_df])\n",
        "cuisine_spec_df = cuisine_spec_df.set_index('cuisines')\n",
        "print('Most preferred Dishes For each cuisines')\n",
        "cuisine_spec_df.head(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npunlB0uUO_p"
      },
      "source": [
        "Classification Analysis for Returants/cafes , I will use Kmean for its performance and simplicity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U8wXKenIfQl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(encoded_df.corr(),annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gwc0rQtycig"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "temp_df = encoded_df[['book_table','dish_liked','rest_type','cuisines','votes','approx_cost(for two people)']].copy(deep=True)\n",
        "scaler = StandardScaler()\n",
        "temp_df = scaler.fit_transform(temp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQaSIUsXe-re"
      },
      "outputs": [],
      "source": [
        "from kmodes.kmodes import KModes\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score,calinski_harabasz_score\n",
        "\n",
        "silhouette_avg_score=[]\n",
        "wcss = []\n",
        "for n_clusters in range(2,8):\n",
        "    # n_init=50 to ensure i got same result everytime\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=50,random_state=42)\n",
        "    #kmeans = KModes(n_clusters=n_clusters, init='Huang', n_init=50,random_state=42)\n",
        "    #kmeans.fit(temp_df)\n",
        "    #clusters = kmeans.predict(temp_df)\n",
        "    clusters = kmeans.fit_predict(temp_df)\n",
        "    silhouette_avg = silhouette_score(temp_df, clusters)\n",
        "    silhouette_avg_score.append(silhouette_avg)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg, \"Inertia\",kmeans.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0bsZqaEhhvT"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(2,8),wcss)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik9I-i_SUOGT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "km = KMeans(init='k-means++', n_clusters = 5, n_init=50,random_state=42)\n",
        "#km = KModes(n_clusters=n_clusters, init='Huang', n_init=5,random_state=42)\n",
        "clusters = km.fit_predict(temp_df)\n",
        "df['cluster'] = clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "549NJ8QCb02L"
      },
      "outputs": [],
      "source": [
        "df[['cluster','approx_cost(for two people)']].groupby('cluster').median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAziZyk9Zm5r"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x='cluster',y='approx_cost(for two people)',data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zkfp5D8oty_"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x='listed_in(type)',y='approx_cost(for two people)',data=df[df.cluster==0][['listed_in(type)','approx_cost(for two people)','votes','weighted_rating']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsEUBQ1MciTJ"
      },
      "outputs": [],
      "source": [
        "multi_bar_chart(df[df.cluster==1][['listed_in(type)','approx_cost(for two people)','votes','weighted_rating']].groupby('listed_in(type)').mean().sort_values(by='approx_cost(for two people)'),'Approximate Cost and Weighted Rating by Cuisines','location',figuresize=(25,10),freq=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EttIOsUZhT4"
      },
      "source": [
        "PCA Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7TnGzyNYD8i"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(temp_df)\n",
        "pca_samples = pca.transform(temp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwZ8-dwZZrVS"
      },
      "outputs": [],
      "source": [
        "def pca_plot(pca_samples, temp_df):\n",
        "  fig, ax1 = plt.subplots(figsize=(14, 5))\n",
        "  sns.set(font_scale=1)\n",
        "\n",
        "  # Plot the individual explained variance on the primary y-axis (right)\n",
        "  sns.barplot(\n",
        "      x=np.arange(0, temp_df.shape[1]),\n",
        "      y=pca.explained_variance_ratio_ * 100,\n",
        "      alpha=0.5,\n",
        "      color='g',\n",
        "      label='Individual explained variance',\n",
        "      ax=ax1\n",
        "  )\n",
        "\n",
        "  # Create a secondary y-axis (left) for the cumulative explained variance\n",
        "  ax2 = ax1.twinx()\n",
        "\n",
        "  # Calculate the cumulative explained variance\n",
        "  cumsum_explained_variance = np.cumsum(pca.explained_variance_ratio_) * 100\n",
        "\n",
        "  # Plot the cumulative explained variance on the secondary y-axis\n",
        "  ax2.plot(\n",
        "      np.arange(0, temp_df.shape[1]),\n",
        "      cumsum_explained_variance,\n",
        "      marker='o',\n",
        "      color='b',\n",
        "      label='Cumulative explained variance'\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Labels and legend for the primary y-axis (individual variance)\n",
        "  ax1.set_ylabel('Explained variance (%)', fontsize=14, color='g')\n",
        "  ax1.set_xlabel('Principal components', fontsize=14)\n",
        "  ax1.legend(loc='upper left', fontsize=13)\n",
        "\n",
        "  # Labels and legend for the secondary y-axis (cumulative variance)\n",
        "  ax2.set_ylabel('Cumulative explained variance (%)', fontsize=14, color='b')\n",
        "  ax2.legend(loc='upper right', fontsize=13)\n",
        "\n",
        "  # Adjust the layout to make room for both y-axes\n",
        "  fig.tight_layout()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "pca_plot(pca_samples, temp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yfDLlHRaF3u"
      },
      "outputs": [],
      "source": [
        "pca = PCA(5)\n",
        "pca.fit(temp_df)\n",
        "pca_samples = pca.transform(temp_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP0KvcYiC2fa"
      },
      "source": [
        "# Visualize pcs in 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnT9ItWZb4C1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_pca_scatter(pca_samples, labels, label_color_map=None, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Creates scatter plots for all pairs of PCA components.\n",
        "\n",
        "    Parameters:\n",
        "    - pca_samples: 2D array or DataFrame containing the PCA components.\n",
        "    - labels: Array or Series containing the cluster labels for color coding.\n",
        "    - label_color_map: Dictionary mapping cluster labels to colors. Default is None, which uses a default color map.\n",
        "    - alpha: Transparency level for the scatter plot points. Default is 0.4.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the style and context for the plot\n",
        "    sns.set_style(\"white\")\n",
        "    sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n",
        "\n",
        "    # If no color map is provided, create a default one\n",
        "    if label_color_map is None:\n",
        "        unique_clusters = np.unique(labels)\n",
        "        default_colors = sns.color_palette(\"hsv\", len(unique_clusters))\n",
        "        label_color_map = {cluster: color for cluster, color in zip(unique_clusters, default_colors)}\n",
        "\n",
        "    # Get colors for each point\n",
        "    label_color = [label_color_map[l] for l in labels]\n",
        "\n",
        "    # Create subplots\n",
        "    n_components = pca_samples.shape[1]\n",
        "    fig, axes = plt.subplots(n_components, n_components, figsize=(15, 15))\n",
        "\n",
        "    # Loop through all pairs of PCA components and create scatter plots\n",
        "    for i in range(n_components):\n",
        "        for j in range(n_components):\n",
        "            ax = axes[i, j]\n",
        "            if i == j:\n",
        "                ax.text(0.5, 0.5, f'PCA {i+1}',\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',\n",
        "                        fontsize=12)\n",
        "                ax.set_visible(False)\n",
        "            else:\n",
        "                ax.scatter(pca_samples[:, i], pca_samples[:, j], c=label_color, alpha=alpha)\n",
        "                ax.set_xlabel(f'PCA {i+1}', fontsize=12)\n",
        "                ax.set_ylabel(f'PCA {j+1}', fontsize=12)\n",
        "                ax.yaxis.grid(color='lightgray', linestyle=':')\n",
        "                ax.xaxis.grid(color='lightgray', linestyle=':')\n",
        "                ax.spines['right'].set_visible(False)\n",
        "                ax.spines['top'].set_visible(False)\n",
        "\n",
        "    # Set layout and show plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_pca_scatter(pca_samples, df['cluster'].values )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgoSSkYeMiCe"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n",
        "    plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "\n",
        "    fig, ax1 = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(8, 8)\n",
        "    ax1.set_xlim([lim_x[0], lim_x[1]])\n",
        "    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "\n",
        "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        cmap = cm.get_cmap(\"Spectral\")\n",
        "        color = cmap(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                           facecolor=color, edgecolor=color, alpha=0.8)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n",
        "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5HjuKF8MuXY"
      },
      "outputs": [],
      "source": [
        "sample_silhouette_values = silhouette_samples(temp_df, clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1knSBD6NEds"
      },
      "outputs": [],
      "source": [
        "graph_component_silhouette(len(pd.Series(clusters).value_counts()), [-0.07, 0.7], len(temp_df), sample_silhouette_values, clusters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VwJYBUDNd8"
      },
      "source": [
        "It Show that clustring was good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPlaL3soGCv8"
      },
      "outputs": [],
      "source": [
        "df['classes'] = df['cluster'].map({0:'Mid',1:'Low',2:'Upper-Mid',3:'Lower-High',4:'High'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndJfmE5nIMMc"
      },
      "outputs": [],
      "source": [
        "encoded_df['classes'] = df['classes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NXv-mZSIGT9"
      },
      "outputs": [],
      "source": [
        "encoded_df['count']=df['count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLZ6FIwLhdIG"
      },
      "outputs": [],
      "source": [
        "radar_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_U0Mq48GiGO"
      },
      "outputs": [],
      "source": [
        "radar_df = encoded_df[['online_order','book_table','weighted_rating','approx_cost(for two people)','num_cuisines','num_spec','count','classes','votes']].groupby('classes').mean().sort_values(by='votes')\n",
        "radar_df = radar_df.rename(columns={'approx_cost(for two people)':'Price','weighted_rating':'Rating','votes':'Engagement','count':'Traffic','num_cuisines':'Variety in Dishes','num_spec':'Variety in Specializations'})\n",
        "radar_df = radar_df.sort_values('Price',ascending=False)\n",
        "radar_df.drop_duplicates(inplace=True)\n",
        "\n",
        "plot_radar_charts(radar_df,single_chart=False,custom_title='Resturants Characteristics per Classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j2b1piykaZY"
      },
      "source": [
        "# Investment Analysis Report: Restaurant Markets in Bangalore\n",
        "\n",
        "## Introduction\n",
        "This report provides an analysis of the restaurant markets across various locations in Bangalore, focusing on customer characteristics, economic conditions, and investment opportunities. Bangalore's diverse culinary landscape offers multiple opportunities for both local and foreign cuisines.\n",
        "\n",
        "## Yeshwantpur\n",
        "\n",
        "**Customer Characteristics:**\n",
        "- **Cuisine Preference:** Predominantly local cuisines.\n",
        "- **Price Sensitivity:** Budget-friendly restaurants.\n",
        "- **Rating:** Below average, indicating potential for improvement with higher quality offerings.\n",
        "- **Consumer Behavior:** Customers seek diverse and affordable food options.\n",
        "\n",
        "**Economic Context:**\n",
        "- Yeshwantpur is a developing residential and commercial area with a mix of affordable and mid-range options. The increasing development is contributing to a dynamic food scene.\n",
        "- **Source:** [Economic Overview of Yeshwantpur](https://www.economictimes.indiatimes.com/industry/services/property-/-cstruction/economic-overview-of-yeshwantpur/articleshow/71130025.cms)\n",
        "\n",
        "**Recommendation:**\n",
        "- New restaurants offering high-quality food can compete effectively due to the current low ratings. Focus on delivering quality at competitive prices to attract budget-conscious yet discerning customers.\n",
        "\n",
        "## Whitefield\n",
        "\n",
        "**Customer Characteristics:**\n",
        "- **Cuisine Preference:** Mainly Asian and European cuisines.\n",
        "- **Price Sensitivity:** Mid-range to higher end.\n",
        "- **Rating:** Average, suggesting a stable but competitive market.\n",
        "- **Consumer Behavior:** Customers seek diverse and international food experiences.\n",
        "\n",
        "**Economic Context:**\n",
        "- Whitefield is a significant IT and residential hub with a higher average income level. It attracts professionals and expatriates, influencing the demand for diverse and high-quality cuisines.\n",
        "- **Source:** [Whitefield Economic and Demographic Insights](https://www.financialexpress.com/industry/whitefield-economic-and-demographic-insights/)\n",
        "\n",
        "**Recommendation:**\n",
        "- Restaurants should focus on offering high-quality Asian and European cuisines to stand out in a competitive market. Emphasize unique and high-quality international dishes to appeal to the area's diverse clientele.\n",
        "\n",
        "## Ulsoor\n",
        "\n",
        "**Customer Characteristics:**\n",
        "- **Cuisine Preference:** Predominantly European cuisines.\n",
        "- **Price Sensitivity:** Upper-middle class.\n",
        "- **Rating:** Generally good, reflecting a well-established market.\n",
        "- **Consumer Behavior:** Customers prefer European cuisines and upscale dining experiences.\n",
        "\n",
        "**Economic Context:**\n",
        "- Ulsoor is an upscale locality known for its affluent residents and historical significance. It has a strong demand for high-quality European cuisines.\n",
        "- **Source:** [Ulsoor Economic and Demographic Profile](https://www.business-standard.com/article/economy-policy/ulsoor-economic-and-demographic-profile-120092700094_1.html)\n",
        "\n",
        "**Recommendation:**\n",
        "- Invest in European cuisine with a focus on premium dining experiences. Offer exceptional dishes and a refined atmosphere to attract the upper-middle-class clientele.\n",
        "\n",
        "## St. Marks Road, MG Road, and Church Street\n",
        "\n",
        "**Customer Characteristics:**\n",
        "- **Cuisine Preference:** Diverse, seeking unique and high-quality experiences.\n",
        "- **Price Sensitivity:** Lower high class.\n",
        "- **Rating:** Generally good, indicating a well-established market.\n",
        "- **Consumer Behavior:** Customers look for both uniqueness and quality in their dining experiences.\n",
        "\n",
        "**Economic Context:**\n",
        "- These areas are central business districts with high-end retail and dining establishments. They cater to a diverse, high-income clientele seeking premium experiences.\n",
        "- **Source:** [Economic Overview of Bangalore's Central Business Districts](https://www.thehindubusinessline.com/news/economic-overview-of-bangalores-central-business-districts/article33585535.ece)\n",
        "\n",
        "**Recommendation:**\n",
        "- Focus on offering unique dining experiences with a high standard of quality to meet the expectations of the lower high-class demographic. Develop innovative menus and distinctive concepts to attract high-income customers.\n",
        "\n",
        "## Lavelle Road\n",
        "\n",
        "**Customer Characteristics:**\n",
        "- **Cuisine Preference:** High-class, with an emphasis on exceptional experiences and uniqueness.\n",
        "- **Price Sensitivity:** High-end.\n",
        "- **Rating:** Very high, indicating a well-established and competitive market.\n",
        "- **Consumer Behavior:** Customers seek exclusivity and high-quality experiences.\n",
        "\n",
        "**Economic Context:**\n",
        "- Lavelle Road is one of Bangalores most affluent areas, known for its upscale residential and commercial properties. It attracts a high-income clientele with a preference for luxury dining.\n",
        "- **Source:** [Lavelle Road Economic Profile](https://www.forbesindia.com/article/real-estate/lavelle-road-economic-profile/57415/1)\n",
        "\n",
        "**Recommendation:**\n",
        "- Invest in high-end dining establishments that offer exceptional experiences and exclusive menu items. Position restaurants as luxury dining destinations with superior service and unique culinary offerings.\n",
        "\n",
        "## Conclusion\n",
        "Bangalore presents a vibrant food scene with opportunities for investment in both foreign and local cuisines. While foreign cuisines like Cantonese, German, Singaporean, and African show promising potential due to their unique appeal and engagement, local cuisines, particularly Northern and Southern Indian, face market saturation and require innovative approaches to remain competitive. Understanding each area's economic conditions and customer characteristics will be key to making informed investment decisions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8uBArbkpri7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRjygSxXphGW"
      },
      "source": [
        "# Investment Report: Strategic Recommendations for Restaurant Ventures in Bangalore\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This report outlines key findings and strategic recommendations for investing in Bangalore's diverse restaurant market. By understanding customer preferences and leveraging neighborhood characteristics, investors can optimize profitability and market positioning.\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### 1. Restaurant Categories and Distribution\n",
        "\n",
        "- **Quick Bites (40%)**: Fast-food and quick-service restaurants dominate but show lower customer engagement. Enhancing the dining experience could boost interest.\n",
        "- **Casual Dining and Cafes**: These segments, which constitute about two-thirds of the market, balance convenience with ambiance, resulting in higher engagement.\n",
        "- **Irani Cafes**: Known for their engaging atmosphere and reasonable pricing, Irani cafes are a growing niche with limited competition.\n",
        "\n",
        "### 2. Performance of Different Categories\n",
        "\n",
        "- **Top Performers**: Drinks & Nightlife establishments attract high engagement and positive ratings. Investing in lively, social venues aligns with consumer preferences.\n",
        "- **Lower Performers**: Delivery, Dine-out, and Desserts categories show lower engagement. Improving service quality and offering unique experiences can address these shortcomings.\n",
        "\n",
        "### 3. Neighborhood Insights\n",
        "\n",
        "- **Yeshwantpur**: Offers affordable and diverse food options, appealing to the middle class.\n",
        "- **Wilson Garden**: Features budget-friendly fast food, attracting cost-conscious diners.\n",
        "- **Whitefield**: Focuses on a mid-to-high budget range with a nightlife emphasis, appealing to upper-middle-class and high-class patrons.\n",
        "- **West Bangalore**: Known for food trucks, catering to a broad audience with diverse tastes.\n",
        "- **Sankey Road and Lavelle Road**: High-end dining options attract affluent customers. **Lavelle Road, in particular, serves as an ideal middle ground, catering to both high-class and upper-middle-class customers. This makes it a strategic location with significant potential for a diverse customer base.**\n",
        "- **Kalyan Nagar**: Provides a balanced dining experience for upper-middle-class residents.\n",
        "\n",
        "### 4. Cuisine-Specific Trends\n",
        "\n",
        "- **Cantonese Cuisine**: High engagement and premium pricing suggest strong demand among affluent customers for exclusive dining experiences.\n",
        "- **German Cuisine**: Moderate engagement with competitive pricing appeals to middle-income groups.\n",
        "- **Sri Lankan, Parsi, and Russian Cuisines**: High engagement and medium pricing highlight a growing interest in unique flavors and cultural diversity.\n",
        "- **Singaporean Cuisine**: Shows increasing popularity with moderate pricing, indicating potential for greater engagement through targeted marketing.\n",
        "- **Chinese Cuisine**: Despite lower engagement, Chinese cuisine is widespread across the city, similar to local cuisines. Its affordability and broad presence suggest room for growth by enhancing uniqueness and quality.\n",
        "- **African Cuisine**: Low engagement but potential for growth by focusing on health-conscious and adventurous diners.\n",
        "\n",
        "## Strategic Recommendations\n",
        "\n",
        "### 1. Pricing Strategy\n",
        "\n",
        "- **Consideration for Price Reduction**: Adjust prices strategically to enhance competitiveness and appeal. Specialize in unique offerings to justify premium pricing where feasible.\n",
        "- **Location-Based Pricing**: Avoid high prices in high-traffic road locations. Instead, focus on specialty areas where a balance between quality and price can attract middle and upper-middle-class customers.\n",
        "\n",
        "### 2. Location Strategy\n",
        "\n",
        "- **Prime Locations**: Invest in high-visibility areas for Drinks & Nightlife venues to attract a diverse clientele.\n",
        "- **Avoid High-Rent Roads**: Opt for locations with manageable rental costs to avoid passing on high prices to customers, which could affect engagement.\n",
        "- **Lavelle Road**: Leverage Lavelle Roads unique position as a middle ground for high-class and upper-middle-class customers. Its balanced demographic profile offers a strategic advantage for attracting a broad customer base.\n",
        "\n",
        "### 3. Enhancing Customer Experience\n",
        "\n",
        "- **Specialization**: Focus on unique dining experiences that offer value and quality. Specialized menus and personalized services can improve customer satisfaction and loyalty.\n",
        "- **Encouraging Reviews**: Actively encourage customers to leave reviews and ratings. Positive feedback not only improves visibility but also drives profitability through increased customer trust and engagement.\n",
        "\n",
        "### 4. Metrics and Analysis\n",
        "\n",
        "- **Key Metrics**: Track and analyze reviews, votes, ratings, and overall customer feedback. These metrics are crucial for assessing customer satisfaction and identifying areas for improvement.\n",
        "- **Direct Profit Correlation**: Higher ratings and positive reviews correlate with increased profitability. Prioritize strategies that enhance customer experience and encourage positive feedback.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Investing in Bangalore's restaurant market requires a strategic approach to pricing, location, and customer engagement. By aligning restaurant concepts with neighborhood characteristics and enhancing customer experience, investors can optimize their returns. **Lavelle Roads strategic position offers significant potential for attracting a varied and high-value customer base.** Additionally, addressing the unique attributes of different cuisines and focusing on customer feedback will drive growth and profitability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY8GnzsrKt61"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehc_hlxE-Yen"
      },
      "source": [
        "# Behavioral Analysis of Restaurant Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG_FANXu-vAQ"
      },
      "source": [
        "# Preparing Data from Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlRphvjd_vgu"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "from nltk.corpus import stopwords, words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvTBXJWCAN-c"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "valid_words = set(words.words())\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS5KyoIJbx_2"
      },
      "source": [
        "Creating a column that contains a list of sentiment analyses for each review involves specifying the sentiment for each sentence and extracting the nouns, as they are likely the aspects the reviewer liked or disliked.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npOoiTk7zcx3"
      },
      "source": [
        "For better accuracy, it would be ideal to use an LLM model for this task, but it would be time-consuming. Therefore, I'll use a pre-trained model from Hugging Face to strike a balance between speed and accuracy. I'll reserve the LLM model for later when dealing with a single restaurant or a smaller number of reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcuiZCLUKhLJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    doc = nlp(clean_string(text))\n",
        "    return [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "def clean_string(s):\n",
        "    # Define allowed characters: alphanumeric, punctuation, and space\n",
        "    allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
        "\n",
        "    # Remove characters not in allowed_chars\n",
        "    cleaned = ''.join(c for c in s if c in allowed_chars)\n",
        "\n",
        "    # Add a space before and after punctuation\n",
        "    cleaned = re.sub(r'([' + re.escape(string.punctuation) + r'])', r' \\1 ', cleaned)\n",
        "\n",
        "    # Add a space between numbers and words if there is no space between them\n",
        "    cleaned = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', cleaned)\n",
        "    cleaned = re.sub(r'([a-zA-Z])(\\d)', r'\\1 \\2', cleaned)\n",
        "\n",
        "    # Replace multiple whitespace with a single space\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "    # Replace multiple consecutive periods with a single period\n",
        "    cleaned = re.sub(r'\\.{2,}', '.', cleaned)\n",
        "\n",
        "    # Remove specific unwanted characters and terms\n",
        "    cleaned = cleaned.replace('','').replace('','').replace('RATED','').replace('but','.').strip()\n",
        "\n",
        "    return cleaned\n",
        "def get_sentiment_scores(df):\n",
        "  df = df.reset_index(drop=True)\n",
        "  df['review_sentiment_list']=None\n",
        "  for indx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    reviews = row['reviews_list']\n",
        "    if len(reviews)==0:\n",
        "      continue\n",
        "    review_sentiment_list = []\n",
        "    for i in reviews:\n",
        "      cleaned_text = re.sub(r'\\s+', ' ', clean_string(i[1]))\n",
        "\n",
        "      doc = nlp(cleaned_text)\n",
        "      sents_sentiment_list = []\n",
        "      sentemints_score_list = []\n",
        "\n",
        "      for sent in doc.sents:\n",
        "        try:\n",
        "          sentiment = sia.polarity_scores(str(sent))\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "        output_dict = None\n",
        "        sentiment_score = sentiment['compound']\n",
        "\n",
        "        sentemints_score_list.append(sentiment_score)\n",
        "        output_dict = {'sentiment_score':sentiment_score,'nouns':[],'adj':[],'sent':str(sent)}\n",
        "\n",
        "        for token in sent:\n",
        "          if token.is_stop:\n",
        "            continue\n",
        "          if len(token.text)>15:\n",
        "            continue\n",
        "          conditions =  token.is_alpha and token.lemma_.lower() in valid_words and token.lemma_.lower() not in stop_words\n",
        "          if token.pos_ == 'NOUN' and conditions :\n",
        "            if sentiment['compound']>=0:\n",
        "              output_dict['nouns'].append(str(token.lemma_.lower()))\n",
        "            elif sentiment['compound']<0:\n",
        "              output_dict['nouns'].append(str(token.lemma_.lower()))\n",
        "          if token.pos_ == 'ADJ' and conditions:\n",
        "            if sentiment['compound']>=0:\n",
        "              output_dict['adj'].append(str(token.lemma_.lower()))\n",
        "            elif sentiment['compound']<0:\n",
        "              output_dict['adj'].append(str(token.lemma_.lower()))\n",
        "\n",
        "        sents_sentiment_list.append(output_dict)\n",
        "      review_sentiment_list.append(sents_sentiment_list)\n",
        "\n",
        "    df.at[indx,'review_sentiment_list'] = review_sentiment_list\n",
        "\n",
        "  return df\n",
        "\n",
        "df=get_sentiment_scores(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSAMMX21gsvL"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/My Drive/Zomato Geospatial Analysis/sentiment_df_spacy.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZqZqtfsLsrX"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/Zomato Geospatial Analysis/sentiment_df_spacy.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4N2eOgZuiwsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N-k2_0XNGPE"
      },
      "outputs": [],
      "source": [
        "# Fix review_sentiment_list column and pase it into python list\n",
        "df['review_sentiment_list'] = df['review_sentiment_list'].apply(lambda x: None if pd.isnull(x) else eval(x))\n",
        "\n",
        "# Fix review_list column and pase it into python list\n",
        "df.reviews_list = df.reviews_list.apply(lambda x: eval(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GQBu1mSQeXZ"
      },
      "outputs": [],
      "source": [
        "nouns_list = []\n",
        "for i in df.review_sentiment_list.tolist():\n",
        "  if i is None:\n",
        "    continue\n",
        "  for j in i:\n",
        "    for k in j:\n",
        "      nouns_list.extend(k['nouns'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB0Vr-VLbWCO"
      },
      "outputs": [],
      "source": [
        "nouns_list = list(set(nouns_list))\n",
        "nouns_list = [s for s in nouns_list if len(s) >= 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcr4q5m9tjDk"
      },
      "outputs": [],
      "source": [
        "len(nouns_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfVeHKcwDkSH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "null=None\n",
        "YOUR_API_KEY = \"AIzaSyDyWlUqOb7TRCdoy3QPf9ABwK7HRAlv4kI\"\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "count = 0\n",
        "def llm(prompt):\n",
        "  while True:\n",
        "    try:\n",
        "      response = model.generate_content(prompt)\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "  return response.text\n",
        "def analays_reviews(reviews_list,word,sentiment):\n",
        "  output = pd.DataFrame(columns=['sentiment'])\n",
        "  sentiment_list = []\n",
        "  for i in reviews_list:\n",
        "\n",
        "    time.sleep(1)\n",
        "    #prompt = f'this is a review on returant \"{clean_string(i)}\" act as data analyst and return object has sentiment analysis related to {word} like these examples sentiment, followed by list of aspect in the sentence followed by reason explaining this aspect {{ \"sentiment\": \"positive\", \"liked\": [{{\"aspect\": \"food\", \"reason\": \"good\"}}, {{\"aspect\": \"delivery\", \"reason\": \"fast\"}}], \"disliked\": [] }} and if it negative {{ \"sentiment\": \"negative\", \"liked\": [], \"disliked\": [{{\"aspect\": \"Food\", \"reason\": \"mediocre\"}}, {{\"aspect\": \"buffet\", \"reason\": \"Only vegetarian\"}}] }} and {{ \"sentiment\": \"mixed\", \"liked\": [{{\"aspect\": \"rice bowl\", \"reason\": \"good\"}}, {{\"aspect\": \"rice bowl\", \"reason\": \"must try\"}}], \"disliked\": [{{\"aspect\": \"pizza\", \"reason\": \"not freshly\"}}, {{\"aspect\": \"pizza\", \"reason\": \"mayonnaise instead of cheese\"}}] }} and {{ \"sentiment\": \"mixed\", \"liked\": [{{\"aspect\": \"chicken pepper fry\", \"reason\": \"good\"}}, {{\"aspect\": \"neer dosa\", \"reason\": \"Soft, fluffy and light\"}}], \"disliked\": [{{\"aspect\": \"chicken ghee roast\", \"reason\": \"lacked the punch\"}}] }} and {{ \"sentiment\": \"mixed\", \"liked\": [{{\"aspect\": \"mushroom\", \"reason\": \"tasty\"}}, {{\"aspect\": \"fruit tart\", \"reason\": \"good\"}}], \"disliked\": [{{\"aspect\": \"food\", \"reason\": \"could have been better\"}}, {{\"aspect\": \"staff\", \"reason\": \"unfriendly\"}}, {{\"aspect\": \"price\", \"reason\": \"steep\"}}] }} in general avoid adding any word won add any meaning minimize number of words and letter.'\n",
        "    prompt = f'''Do aspect sentiment analysis on this review \"{clean_string(i)}\" and return these\n",
        "    1- Sentiment\n",
        "    2- reson for each aspect reviewer liked or disliked\n",
        "    3- list of thing reviewer liked, like of thing reviewer disliked\n",
        "    4- only interested in {sentiment} sentiment related to this word {word}\n",
        "    5- output should be in python dict format\n",
        "    here are some examples\n",
        "    {{\"sentiment\": \"positive\", \"liked\": [{{\"aspect\": \"food\", \"reason\": \"good\"}}, {{\"aspect\": \"delivery\", \"reason\": \"fast\"}}], \"disliked\": [] }}\n",
        "    {{ \"sentiment\": \"negative\", \"liked\": [], \"disliked\": [{{\"aspect\": \"food\", \"reason\": \"mediocre\"}}, {{\"aspect\": \"buffet\", \"reason\": \"Only vegetarian\"}}] }}\n",
        "    {{ \"sentiment\": \"negative\", \"liked\": [], \"disliked\": [{{\"aspect\": \"place\", \"reason\": \"expensive\"}}, {{\"aspect\": \"ambiance\", \"reason\": \"noisy\"}}] }}\n",
        "\n",
        "     '''\n",
        "    printit=True\n",
        "    while printit:\n",
        "      try:\n",
        "        response = model.generate_content(prompt)\n",
        "        printit = False\n",
        "      except:\n",
        "        pass\n",
        "    try:\n",
        "      encoded = eval(response.text)\n",
        "      if encoded['sentiment'] == sentiment:\n",
        "        sentiment_list.append(encoded)\n",
        "        print(encoded)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "  output.sentiment=sentiment_list\n",
        "  return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cb7Yu4OoY8y"
      },
      "outputs": [],
      "source": [
        "# Create the figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot the first line on the primary y-axis\n",
        "sns.lineplot(data=df, x='approx_cost(for two people)', y='num_reviews', ax=ax1, color='blue', label='Number of Reviews')\n",
        "ax1.set_xlabel('Approximate Cost (for two people)')\n",
        "ax1.set_ylabel('Number of Reviews', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# Create a second y-axis and plot the second line\n",
        "ax2 = ax1.twinx()\n",
        "df_counts = df[['name', 'approx_cost(for two people)']].groupby('approx_cost(for two people)').count().reset_index()\n",
        "sns.lineplot(data=df_counts, x='approx_cost(for two people)', y='name', ax=ax2, color='orange', label='Count of Restaurants')\n",
        "sns.lineplot(data=df, x='approx_cost(for two people)', y=df['weighted_rating'] * 500, ax=ax2, color='red', label='Scaled Weighted Rating')\n",
        "ax2.set_ylabel('Count of Restaurants', color='orange')\n",
        "ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "# Set the title and grid\n",
        "plt.title('Number of Reviews and Count of Restaurants vs. Approximate Cost')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Add vertical lines at each 100 unit interval on the x-axis\n",
        "for x in range(0, int(df['approx_cost(for two people)'].max()) + 100, 100):\n",
        "    ax1.axvline(x=x, color='gray', linestyle='--', linewidth=0.7)\n",
        "\n",
        "# Add legends\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxPOItGdjS-m"
      },
      "outputs": [],
      "source": [
        "nouns_list = []\n",
        "for i in df.review_sentiment_list.tolist():\n",
        "  if i is None:\n",
        "    continue\n",
        "  for j in i:\n",
        "    for k in j:\n",
        "      nouns_list.extend(k['nouns'])\n",
        "nouns_list= pd.DataFrame(nouns_list,columns=['word']).word.value_counts().reset_index()\n",
        "nouns_list.columns = ['word','count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfoo6koxpaHI"
      },
      "outputs": [],
      "source": [
        "def word_cloud(nouns_list,col='word'):\n",
        "  most_freq = nouns_list.loc[:100]\n",
        "  word_freq = dict(zip(most_freq[col], most_freq['count']))\n",
        "\n",
        "  # Create and generate a word cloud image\n",
        "  wordcloud = WordCloud(width=1200, height=600, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "  # Display the generated image\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis('off')  # Hide axes\n",
        "  plt.show()\n",
        "word_cloud(nouns_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOTW2Wcj0IYh"
      },
      "source": [
        "\n",
        "In analyzing customer behavior in Bengaluru's restaurant industry, our findings reveal that the number of reviews is skewed toward higher-end establishments, which is expected since budget restaurants typically prioritize affordability over experience and quality. To accurately compare customer sentiments across different restaurant categories, it's essential to set a clear price range that defines each class.\n",
        "\n",
        "Interestingly, there's a noticeable drop in the number of reviews in the range of approximately 1,500 to 2,300 reviews. This unusual pattern warrants further investigation to understand its underlying causes.\n",
        "\n",
        "Additionally, word cloud analysis shows that the overall experience is as crucial as the food itself, sometimes even more so, particularly in higher-end establishments. However, it's important to note that these conclusions are primarily relevant to higher-class restaurants due to the bias in the data toward these types of establishments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOv0qctM2WTX"
      },
      "source": [
        "**What classes fall in price range 1700-2300?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtGhdvdEyuIh"
      },
      "outputs": [],
      "source": [
        "temp_df = df[(df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)]\n",
        "\n",
        "# Count the occurrences of each classes\n",
        "temp_df = temp_df['classes'].reset_index()\n",
        "pie_chart(temp_df,'classes','Count of classes with Approximate Cost between 1700 and 2300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03WNFsvO--_G"
      },
      "source": [
        "Over 90% of the classes in this range cater to Mid, Upper-Mid, and High-Class groups. However, the engagement and ratings are lower because the area is equally divided between High-class and Mid-class residents. This makes it challenging to meet both groups' expectations in terms of quality and budget.\n",
        "\n",
        "**Need more investigation to confirm it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2x5plr88rr"
      },
      "source": [
        "**Why there is a gap in the middle?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1XHk18m8Pi4"
      },
      "source": [
        "What types of each class are in this range?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE05r0C1JiP4"
      },
      "source": [
        "I will Plot prices with frequancy of each type of each class in this range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0KP5tkv8Oqt"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "temp_df = df[(df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)][['classes','approx_cost(for two people)','listed_in(type)','weighted_rating','votes']].groupby(['classes','listed_in(type)']).mean().reset_index()\n",
        "freq = df[(df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)][['classes','approx_cost(for two people)','listed_in(type)','weighted_rating','votes']].groupby(['classes','listed_in(type)']).count().reset_index()\n",
        "sns.barplot(x='classes', y='approx_cost(for two people)', data=temp_df,hue='listed_in(type)',dodge=True,alpha=0.5,legend=False)\n",
        "ax2 = plt.gca().twinx()\n",
        "sns.lineplot(x='classes', y='votes', data=freq,hue='listed_in(type)',ax=ax2)\n",
        "\n",
        "ax2.legend(loc='upper left', bbox_to_anchor=(1, 0.85))\n",
        "plt.title('Price vs Classes in Price Range 1700-2300 and Frequancy')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-H3nJ0rf8up"
      },
      "source": [
        "High class has the highest share in this price range and Dine-out is the highest among all types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_TqPshzIGK4"
      },
      "source": [
        "Now I will CHeck avg price for High and Mid class prices for each type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2UaYxYmHPG6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Plot for 'High' class\n",
        "temp_df = df[df.classes == 'High'][['approx_cost(for two people)', 'listed_in(type)', 'weighted_rating', 'votes']].groupby(['listed_in(type)']).mean().reset_index()\n",
        "sns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=temp_df, alpha=0.7, label='High')\n",
        "# Plot for 'High' class\n",
        "temp_df = df[df.classes == 'Lower-High'][['approx_cost(for two people)', 'listed_in(type)', 'weighted_rating', 'votes']].groupby(['listed_in(type)']).mean().reset_index()\n",
        "sns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=temp_df, alpha=0.7, label='Lower-High')\n",
        "\n",
        "\n",
        "\n",
        "# Plot for 'Upper-Mid' class\n",
        "temp_df = df[df.classes == 'Upper-Mid'][['approx_cost(for two people)', 'listed_in(type)', 'weighted_rating', 'votes']].groupby(['listed_in(type)']).mean().reset_index()\n",
        "sns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=temp_df,alpha=0.5,color='green', label='Upper-Mid')\n",
        "\n",
        "# Plot for 'Mid' class\n",
        "temp_df = df[df.classes == 'Mid'][['approx_cost(for two people)', 'listed_in(type)', 'weighted_rating', 'votes']].groupby(['listed_in(type)']).mean().reset_index()\n",
        "sns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=temp_df, alpha=0.3,color='orange', label='Mid')\n",
        "\n",
        "# Add a title and labels\n",
        "plt.title('Price vs Classes in Price Over all')\n",
        "plt.xlabel('Listed In (Type)')\n",
        "plt.ylabel('Price')\n",
        "\n",
        "# Add a legend, letting matplotlib handle it automatically\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouKbeEbHDZmo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "temp_df = df[(df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)][['classes','approx_cost(for two people)', 'listed_in(type)', 'weighted_rating', 'votes']].groupby(['classes','listed_in(type)']).mean().reset_index()\n",
        "\n",
        "sns.barplot(x='classes', y='votes', data=temp_df,hue='listed_in(type)',dodge=True)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 0.85))\n",
        "plt.title('Votes vs Classes in Price Range 1700-2300')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Votes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIckkhz8MhP6"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def sentiment_by_type_for_class(df):\n",
        "\n",
        "  df.dropna(subset=['review_sentiment_list'],inplace=True)\n",
        "  temp_df = df.reset_index()\n",
        "  # get listed_in(type) unique values\n",
        "  listed_in_type = temp_df['listed_in(type)'].unique()\n",
        "  # create new column sum all sentiment in each row\n",
        "  temp_df['nouns'] = None\n",
        "  temp_df['adj'] = None\n",
        "  temp_df['sentiment_score']=0\n",
        "  temp_df['sentiment_avg']=0\n",
        "\n",
        "  nouns_dict={}\n",
        "  adj_dict={}\n",
        "  types_dict={}\n",
        "  for i,row in temp_df.iterrows():\n",
        "    expande_reviews = list(itertools.chain(*list(row['review_sentiment_list'])))\n",
        "    temp_ph = [d['sentiment_score'] for d in expande_reviews]\n",
        "    temp_df.at[i,'sentiment_score'] = str(temp_ph)\n",
        "    if len(temp_ph) != 0:\n",
        "      temp_df.at[i,'sentiment_avg'] = sum(temp_ph)/len(temp_ph)\n",
        "    else:\n",
        "      temp_df.at[i,'sentiment_avg'] = 0\n",
        "    temp_df.at[i,'nouns'] = str([d['nouns'] for d in expande_reviews])\n",
        "    temp_df.at[i,'adj'] = str([d['adj'] for d in expande_reviews])\n",
        "\n",
        "  temp_df.sentiment_score = temp_df.sentiment_score.apply(lambda x: eval(x))\n",
        "  temp_df.nouns = temp_df.nouns.apply(lambda x: eval(x))\n",
        "  temp_df.adj = temp_df.adj.apply(lambda x: eval(x))\n",
        "\n",
        "\n",
        "  return temp_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQK8xK0QgoMa"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentiment_df=sentiment_by_type_for_class(df.copy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OHBH7HGE6mU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "sns.barplot(data=sentiment_df[(sentiment_df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)][['classes','listed_in(type)','sentiment_avg']].groupby(['listed_in(type)','classes']).mean().reset_index(),hue='classes', x='listed_in(type)', y='sentiment_avg',alpha=0.5)\n",
        "sns.lineplot(data=sentiment_df[(sentiment_df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)][['classes','listed_in(type)','sentiment_avg']].groupby(['listed_in(type)','classes']).mean().reset_index(),hue='classes', x='listed_in(type)', y='sentiment_avg',linewidth=4)\n",
        "plt.title('Sentiment vs Types per class in Price Range 1700-2300')\n",
        "plt.xlabel('Types')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 0.85))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8oI_XbGi3tY"
      },
      "source": [
        "**What is reviews saying about resturant in this range?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wstN-WWlR68"
      },
      "outputs": [],
      "source": [
        "def exctract_review_price_range(sentiment_df):\n",
        "\n",
        "  temp_df=sentiment_df[['sentiment_score','nouns','adj','classes','listed_in(type)']]\n",
        "  temp_dict={'noun':[],'adj':[],'sentiment':[],'classes':[],'type':[]}\n",
        "  for i,row in temp_df.iterrows():\n",
        "    sent_nouns = row.nouns\n",
        "    sent_adj = row.adj\n",
        "    sent_score = row.sentiment_score\n",
        "    for i_s,s in enumerate(sent_adj):\n",
        "      for n in s:\n",
        "        temp_dict['adj'].append(n)\n",
        "        temp_dict['classes'].append(row.classes)\n",
        "        temp_dict['type'].append(row['listed_in(type)'])\n",
        "        temp_dict['noun'].append(sent_nouns[i_s])\n",
        "        temp_dict['sentiment'].append(sent_score[i_s])\n",
        "\n",
        "\n",
        "\n",
        "  noun_sentiment_df = pd.DataFrame(temp_dict)\n",
        "  noun_sentiment_df=noun_sentiment_df.explode('noun').reset_index(drop=True)\n",
        "\n",
        "  return noun_sentiment_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYGCNWDGieU8"
      },
      "outputs": [],
      "source": [
        "noun_sentiment_1700_2300 = exctract_review_price_range(sentiment_df[(sentiment_df['approx_cost(for two people)'] > 1700) & (df['approx_cost(for two people)'] < 2300)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu20mkab-IWD"
      },
      "source": [
        "Using LLM Model to categorize the lise accurtly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBxxmXfO-QCm"
      },
      "outputs": [],
      "source": [
        "categories = { 'food quality': 'Food Quality', 'food': 'Food Quality', 'dish': 'Food Quality', 'meal': 'Food Quality', 'cuisine': 'Food Quality', 'menu': 'Food Quality', 'taste': 'Food Quality', 'flavor': 'Food Quality', 'presentation': 'Food Quality', 'freshness': 'Food Quality', 'ingredients': 'Food Quality', 'preparation': 'Food Quality', 'taste profile': 'Food Quality', 'texture': 'Food Quality', 'portion size': 'Food Quality', 'recipe': 'Food Quality', 'seasoning': 'Food Quality', 'spices': 'Food Quality', 'dish quality': 'Food Quality', 'meal quality': 'Food Quality', 'food experience': 'Food Quality', 'culinary': 'Food Quality', 'dish presentation': 'Food Quality', 'cooking techniques': 'Food Quality', 'ingredient quality': 'Food Quality', 'food standards': 'Food Quality', 'food preparation': 'Food Quality', 'flavor profile': 'Food Quality', 'meal preparation': 'Food Quality', 'dish preparation': 'Food Quality', 'food safety': 'Food Quality', 'culinary quality': 'Food Quality', 'kitchen standards': 'Food Quality', 'gourmet': 'Food Quality', 'culinary excellence': 'Food Quality', 'menu variety': 'Food Quality', 'dish variety': 'Food Quality', 'menu selection': 'Food Quality', 'service': 'Service', 'staff': 'Service', 'employee': 'Service', 'team': 'Service', 'worker': 'Service', 'attendant': 'Service', 'personnel': 'Service', 'assistant': 'Service', 'waitstaff': 'Service', 'receptionist': 'Service', 'manager': 'Service', 'barista': 'Service', 'server': 'Service', 'host': 'Service', 'hostess': 'Service', 'staff member': 'Service', 'crew': 'Service', 'service quality': 'Service', 'customer service': 'Service', 'service standard': 'Service', 'hospitality': 'Service', 'care': 'Service', 'attention': 'Service', 'support': 'Service', 'assistance': 'Service', 'service staff': 'Service', 'service experience': 'Service', 'service level': 'Service', 'service delivery': 'Service', 'interaction': 'Service', 'customer support': 'Service', 'service efficiency': 'Service', 'employee conduct': 'Service', 'staff behavior': 'Service', 'service team': 'Service', 'staff responsiveness': 'Service', 'team performance': 'Service', 'service attitude': 'Service', 'service provision': 'Service', 'staff professionalism': 'Service', 'staff demeanor': 'Service', 'service excellence': 'Service', 'ambiance': 'Ambiance/Atmosphere', 'atmosphere': 'Ambiance/Atmosphere', 'environment': 'Ambiance/Atmosphere', 'mood': 'Ambiance/Atmosphere', 'setting': 'Ambiance/Atmosphere', 'vibe': 'Ambiance/Atmosphere', 'character': 'Ambiance/Atmosphere', 'tone': 'Ambiance/Atmosphere', 'feel': 'Ambiance/Atmosphere', 'aesthetic': 'Ambiance/Atmosphere', 'energy': 'Ambiance/Atmosphere', 'style': 'Ambiance/Atmosphere', 'theme': 'Ambiance/Atmosphere', 'decor': 'Ambiance/Atmosphere', 'layout': 'Ambiance/Atmosphere', 'sound': 'Ambiance/Atmosphere', 'lighting': 'Ambiance/Atmosphere', 'music': 'Ambiance/Atmosphere', 'furnishings': 'Ambiance/Atmosphere', 'interior': 'Ambiance/Atmosphere', 'exterior': 'Ambiance/Atmosphere', 'design': 'Ambiance/Atmosphere', 'artwork': 'Ambiance/Atmosphere', 'color scheme': 'Ambiance/Atmosphere', 'decorations': 'Ambiance/Atmosphere', 'design elements': 'Ambiance/Atmosphere', 'atmospheric elements': 'Ambiance/Atmosphere', 'ambiance quality': 'Ambiance/Atmosphere', 'ambiance level': 'Ambiance/Atmosphere', 'overall ambiance': 'Ambiance/Atmosphere', 'ambiance setting': 'Ambiance/Atmosphere', 'ambiance experience': 'Ambiance/Atmosphere', 'environmental factors': 'Ambiance/Atmosphere', 'design aspects': 'Ambiance/Atmosphere', 'ambiance design': 'Ambiance/Atmosphere', 'ambiance impact': 'Ambiance/Atmosphere', 'ambiance effect': 'Ambiance/Atmosphere', 'ambiance creation': 'Ambiance/Atmosphere', 'ambiance management': 'Ambiance/Atmosphere', 'ambiance style': 'Ambiance/Atmosphere', 'ambiance theme': 'Ambiance/Atmosphere', 'atmospheric experience': 'Ambiance/Atmosphere', 'atmosphere quality': 'Ambiance/Atmosphere', 'ambiance appeal': 'Ambiance/Atmosphere', 'ambiance influence': 'Ambiance/Atmosphere', 'ambiance characteristics': 'Ambiance/Atmosphere', 'experience': 'Overall Experience', 'overall': 'Overall Experience', 'general experience': 'Overall Experience', 'overall satisfaction': 'Overall Experience', 'visit experience': 'Overall Experience', 'customer experience': 'Overall Experience', 'overall impression': 'Overall Experience', 'overall rating': 'Overall Experience', 'overall quality': 'Overall Experience', 'general impression': 'Overall Experience', 'overall service': 'Overall Experience', 'total experience': 'Overall Experience', 'complete experience': 'Overall Experience', 'experience level': 'Overall Experience', 'comprehensive experience': 'Overall Experience', 'price': 'Price', 'cost': 'Price', 'value': 'Price', 'expensive': 'Price', 'cheap': 'Price', 'affordable': 'Price', 'inexpensive': 'Price', 'value for money': 'Price', 'discount': 'Price', 'price point': 'Price', 'charge': 'Price', 'rate': 'Price', 'fee': 'Price', 'amount': 'Price', 'spending': 'Price', 'costs': 'Price', 'expense': 'Price', 'tab': 'Price', 'bill': 'Price', 'fare': 'Price', 'price range': 'Price', 'price level': 'Price', 'premium': 'Price', 'luxury': 'Price', 'cheapness': 'Price', 'cost-effectiveness': 'Price', 'costliness': 'Price', 'spending power': 'Price', 'affordability': 'Price', 'economic': 'Price', 'discounts': 'Price', 'promotions': 'Price', 'deals': 'Price', 'special offers': 'Price', 'bargains': 'Price', 'savings': 'Price', 'expense level': 'Price', 'financial value': 'Price', 'economic value': 'Price', 'value proposition': 'Price', 'cleanliness': 'Cleanliness', 'hygiene': 'Cleanliness', 'neatness': 'Cleanliness', 'sanitation': 'Cleanliness', 'orderliness': 'Cleanliness', 'tidiness': 'Cleanliness', 'spotless': 'Cleanliness', 'pristine': 'Cleanliness', 'immaculate': 'Cleanliness', 'freshness': 'Cleanliness', 'maintained': 'Cleanliness', 'well-kept': 'Cleanliness', 'clean': 'Cleanliness', 'germ-free': 'Cleanliness', 'disinfection': 'Cleanliness', 'sanitary': 'Cleanliness', 'cleaned': 'Cleanliness', 'scrubbed': 'Cleanliness', 'polished': 'Cleanliness', 'sterile': 'Cleanliness', 'hygienic': 'Cleanliness', 'sanitized': 'Cleanliness', 'uncluttered': 'Cleanliness', 'spick and span': 'Cleanliness', 'orderly': 'Cleanliness', 'fresh': 'Cleanliness', 'dirt-free': 'Cleanliness', 'dust-free': 'Cleanliness', 'scrubbed': 'Cleanliness', 'cleared': 'Cleanliness', 'spic-and-span': 'Cleanliness', 'well-maintained': 'Cleanliness', 'cleaned': 'Cleanliness', 'immaculate': 'Cleanliness', 'special feature': 'Special Features', 'feature': 'Special Features', 'amenity': 'Special Features', 'attraction': 'Special Features', 'unique': 'Special Features', 'perk': 'Special Features', 'highlight': 'Special Features', 'advantage': 'Special Features', 'extra': 'Special Features', 'offering': 'Special Features', 'facility': 'Special Features', 'benefit': 'Special Features', 'additional feature': 'Special Features', 'distinctive': 'Special Features', 'option': 'Special Features', 'addition': 'Special Features', 'value-add': 'Special Features', 'enhancement': 'Special Features', 'specialty': 'Special Features', 'exclusive': 'Special Features', 'novelty': 'Special Features', 'feature set': 'Special Features', 'special offer': 'Special Features', 'bonus': 'Special Features', 'unique offering': 'Special Features', 'extra feature': 'Special Features', 'distinction': 'Special Features', 'exceptional': 'Special Features', 'premium feature': 'Special Features', 'special characteristic': 'Special Features', 'highlighted feature': 'Special Features', 'notable': 'Special Features', 'rare': 'Special Features', 'differentiator': 'Special Features', 'superior': 'Special Features' }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-yufB1Isf0-"
      },
      "outputs": [],
      "source": [
        "def prepar_radar_for_type(df,classes):\n",
        "  df['words'] = df.apply(lambda row: [row['noun'], row['adj']], axis=1)\n",
        "  df = df.explode('words')\n",
        "  df=df.reset_index(drop=True)\n",
        "  df['words'] = df['words'].replace(categories)\n",
        "  int_list = ['Food Quality','Service','Ambiance/Atmosphere','Price','Cleanliness','Special Features']\n",
        "  list_of_type=df['type'].unique()\n",
        "  dff=pd.DataFrame(columns=int_list)\n",
        "  for i in list_of_type:\n",
        "    temp_df=df[df.classes==classes][df['type']==i][['words','sentiment']][df['words'].isin(int_list)].groupby('words').mean().sort_values('sentiment',ascending=False).reset_index()\n",
        "    if len(temp_df)==0:\n",
        "      continue\n",
        "    temp_df.set_index('words',inplace=True)\n",
        "    temp_df = temp_df.T\n",
        "    temp_df.index=[i]\n",
        "    dff=pd.concat([dff,temp_df],axis=0)\n",
        "  return dff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMNdVie71OIK"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_1700_2300.copy(),'High'),single_chart=True,custom_title='Reviews insights for High Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj2zAYYLx_ZZ"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_1700_2300.copy(),'Lower-High'),single_chart=True,custom_title='Reviews insights for Lower-High Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK5fX7PiyEMF"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_1700_2300.copy(),'Upper-Mid'),single_chart=True,custom_title='Reviews insights for Upper-Mid Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMWJ8dgcyWLp"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_1700_2300.copy(),'Mid'),single_chart=True,custom_title='Reviews insights for Mid Class',scalling=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl6eRfRV59pS"
      },
      "source": [
        "### Important Note:\n",
        "\n",
        "The analysis of reviews provided here is not entirely accurate and should not be taken too seriously due to several biases. The analysis is skewed towards specific customer segments because not everyone is interested in leaving reviews. Additionally, the focus was on speed rather than accuracy, so while this can serve as a starting point, it may not cover all aspects of the review data.\n",
        "\n",
        "### Price Range Analysis (1500-2300):\n",
        "\n",
        "For the price range of 1500-2300, we've observed that this segment overlaps with multiple customer classes, making it challenging to meet everyone's expectations. If restaurants aim to improve food quality and the overall experience to satisfy higher-end customers, the prices may become unsatisfactory for lower-end customers, and vice versa.\n",
        "\n",
        "However, within this price range, establishments catering to the lower-high class, particularly in categories like Dine-out, Drinks & Nightlife, and Pubs and Bars, tend to perform better than others. Despite this, the overall review sentiment remains less optimistic compared to other segments, which highlights the complexity of catering to a diverse customer base within this range.\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "For investors interested in this price range, it is crucial to carefully select the location and decor to reflect the target customer class. Additionally, the marketing team should intensify its efforts to ensure that the right customer segment is being targeted effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mapnr0hycx6"
      },
      "source": [
        "**What are Reviews insights about each Class on all price range?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0F36UhRzFv2"
      },
      "outputs": [],
      "source": [
        "noun_sentiment_all_range = exctract_review_price_range(sentiment_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hoaeA7pzS6_"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_all_range.copy(),'High'),single_chart=False,custom_title='Reviews insights for High Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2QIOHo62BsW"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_all_range.copy(),'Lower-High'),single_chart=False,custom_title='Reviews insights for lower-High Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "987jPrZ_2vwr"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_all_range.copy(),'Upper-Mid'),single_chart=False,custom_title='Reviews insights for Upper-Mid Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhRa9EJQ20Z8"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_all_range.copy(),'Mid'),single_chart=False,custom_title='Reviews insights for Mid Class',scalling=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izNJEbms22_q"
      },
      "outputs": [],
      "source": [
        "plot_radar_charts(prepar_radar_for_type(noun_sentiment_all_range.copy(),'Low'),single_chart=False,custom_title='Reviews insights for Low Class',scalling=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5X1wHfAFYqK"
      },
      "source": [
        "### Analysis of Customer Satisfaction by Class and Type\n",
        "\n",
        "#### High Class:\n",
        "For the high class, there is a noticeable **similarity** in customer satisfaction across **Delivery, Cafes, and Drinks & Nightlife** categories. This similarity might be due to the convenience and premium experience these options offer, catering well to the preferences of this segment. However, **Buffet services** stand out with **higher satisfaction levels** compared to other types, possibly because they offer a variety of options that appeal to this class. On the other hand, **Dine-out experiences** show **poor satisfaction**, particularly regarding **pricing**, likely because the high prices may not align with the perceived value for this class.\n",
        "\n",
        "**Recommendations for Investors:**  \n",
        "For investors targeting the high class, **focusing on enhancing the buffet experience** could be promising. Its also advisable to **reconsider pricing strategies for Dine-out** services to better meet the expectations of this segment.\n",
        "\n",
        "#### Lower-High Class:\n",
        "In the lower-high class, there is a similarity in satisfaction levels across **Dine-out, Pubs & Bars, and Drinks & Nightlife** categories, but all of these show **lower satisfaction regarding pricing**. This may be because the pricing in these categories doesn't match the perceived value for this group. **Desserts** also show **poor satisfaction**, indicating a potential area for improvement.\n",
        "\n",
        "**Recommendations for Investors:**  \n",
        "Investors could focus on **improving pricing strategies, food quality, special features, and cleanliness** in Dessert offerings. Additionally, **enhancing the Buffet experience** with better pricing and cleanliness could attract this class.\n",
        "\n",
        "#### Upper-Mid Class:\n",
        "The upper-mid class shows similarities in satisfaction across **Delivery, Cafes, and Dine-out** categories, with generally acceptable satisfaction except for **pricing**. However, **Desserts and Drinks & Nightlife** categories show **poor satisfaction**, particularly regarding **pricing and cleanliness**. The **best satisfaction levels** are seen in **Pubs & Bars and Buffets**.\n",
        "\n",
        "**Recommendations for Investors:**  \n",
        "For this segment, **maintaining the high standards in Pubs & Bars and Buffets** is key. Meanwhile, **improving pricing and cleanliness** in Desserts and Drinks & Nightlife could yield better satisfaction.\n",
        "\n",
        "#### Mid Class:\n",
        "The mid class displays **poor satisfaction** in **Drinks & Nightlife** and medium satisfaction in **Pubs & Bars**. In contrast, **Buffets** receive **high satisfaction**, but **cleanliness** is a concern. For **Drinks & Nightlife and Pubs & Bars**, satisfaction with **food quality, services, and special features** is moderate, suggesting areas for potential improvement.\n",
        "\n",
        "**Recommendations for Investors:**  \n",
        "Investors should focus on **improving cleanliness in Buffets** and **enhancing the overall experience in Drinks & Nightlife and Pubs & Bars**, particularly in food quality and special features.\n",
        "\n",
        "#### Low Class:\n",
        "The low class has the least number of reviews, but the analysis shows **significant satisfaction** in **Delivery, Desserts, Dine-out, Cafes, and Buffets**. However, **Buffet pricing** shows **very poor satisfaction**, and **Dessert pricing** has only **medium satisfaction**. Overall, there is **low satisfaction with Desserts** across all aspects, and **Drinks & similar venues** show **medium to poor satisfaction** with pricing across all classes.\n",
        "\n",
        "**Recommendations for Investors:**  \n",
        "For the low class, investors should focus on **addressing pricing concerns in Buffets and Drinks with all types** and **improving the overall Dessert experience**, including pricing and quality. These could be potential areas for gaining a competitive advantage.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_6EhwhR_rwS"
      },
      "source": [
        "In order to understand i will analysis sentement and aspect for this price range from reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3lqrEer0e0Y"
      },
      "source": [
        "# Part 3 Building Price Prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UqhhTHGwDSA"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arus9jP_aGRN"
      },
      "source": [
        "Get only valid rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyDh-P_xaFyr"
      },
      "outputs": [],
      "source": [
        "valid_df = df[df.is_rate_valid==1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9n8eZg2XRrK"
      },
      "outputs": [],
      "source": [
        "#valid_df['menu_item'] = valid_df['menu_item'].str.replace(r'[^a-zA-Z,\\s]', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EulPxiFX2zS"
      },
      "outputs": [],
      "source": [
        "#separate_types('menu_item')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_mIumWy31G4"
      },
      "source": [
        "Columns need to be deleted because they do not add value to the prediction model, or there are alternative features that were engineered earlier, would cause multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqrmdFHC0lG8"
      },
      "outputs": [],
      "source": [
        "cols = ['url','address','name','rest_type','dish_liked','cuisines','reviews_list','review_sentiment_list','menu_item']\n",
        "pred_df = df.drop(cols,axis=1).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bLCXWCo40y8"
      },
      "outputs": [],
      "source": [
        "pred_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqzlHz0u5pn8"
      },
      "source": [
        "### Feature encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHS3uc8SZkb8"
      },
      "source": [
        "Applying Binary Encoding for columns with yes and no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urdGHuSg46kZ"
      },
      "outputs": [],
      "source": [
        "pred_df['online_order'] = pred_df['online_order'].replace({'Yes': 1, 'No': 0})\n",
        "pred_df['book_table'] = pred_df['book_table'].replace({'Yes': 1, 'No': 0})\n",
        "pred_df['is_new'] = pred_df['is_new'].replace({'Yes': 1, 'No': 0})\n",
        "pred_df['is_road'] = pred_df['is_road'].replace({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMpAqTnfY8Q4"
      },
      "source": [
        "Appplying Ordinal Encoding for Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLEade-6ZDXm"
      },
      "outputs": [],
      "source": [
        "pred_df['classes']=pred_df['classes'].replace({'High': 5, 'Lower-High': 4,'Upper-Mid':3,'Mid':2,'Low':1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df=pred_df.sort_values('approx_cost(for two people)',ascending=False).iloc[1:]"
      ],
      "metadata": {
        "id": "FlKren-5PgLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD31HW0K92zt"
      },
      "outputs": [],
      "source": [
        "def remove_outliers_iqr(df, column_name):\n",
        "    \"\"\"\n",
        "    Removes outliers from a DataFrame column using the IQR method.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame containing the data.\n",
        "    column_name (str): The name of the column to process.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with outliers removed.\n",
        "    \"\"\"\n",
        "    # Ensure the column exists in the DataFrame\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "    Q1 = df[column_name].quantile(0.25)\n",
        "    Q3 = df[column_name].quantile(0.75)\n",
        "\n",
        "    # Calculate IQR\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Determine the lower and upper bounds\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filter out outliers\n",
        "    df_filtered = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "#pred_df = remove_outliers_iqr(pred_df,'approx_cost(for two people)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-5C8wXt6Y_c"
      },
      "source": [
        "Rest Features i will do target encoding becuase i have too many features but important step to avoid data leakage is that i will split my data to train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIln4H-25-Ts"
      },
      "outputs": [],
      "source": [
        "train,test = train_test_split(pred_df,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUuP_BKP8AAY"
      },
      "outputs": [],
      "source": [
        "cols = ['rest_type_0','rest_type_1','cuisines_0','cuisines_1','cuisines_2','cuisines_3','cuisines_4','cuisines_5','cuisines_6','cuisines_7','dish_liked_0','dish_liked_1','dish_liked_2','dish_liked_3','dish_liked_4','dish_liked_5','dish_liked_6','listed_in(type)','listed_in(city)','location']\n",
        "train_encoded, feature_mappings = target_encode_multiple_columns_with_smoothing(cols, train,'approx_cost(for two people)', smoothing=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded.fillna(0,inplace=True)"
      ],
      "metadata": {
        "id": "f0o9wgC_dHAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTs5tNBq-1vx"
      },
      "outputs": [],
      "source": [
        "for column_name, mapping_dict in feature_mappings.items():\n",
        "    test[column_name] = test[column_name].map(mapping_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY7m3fd1QrUN"
      },
      "outputs": [],
      "source": [
        "cols=['rest_type_0','rest_type_1']\n",
        "train_encoded = sum_columns_and_delete(train_encoded,cols,'rest_type')\n",
        "test = sum_columns_and_delete(test,cols,'rest_type')\n",
        "\n",
        "cols=['cuisines_0','cuisines_1','cuisines_2','cuisines_3','cuisines_4','cuisines_5','cuisines_6','cuisines_7']\n",
        "train_encoded = sum_columns_and_delete(train_encoded,cols,'cuisines')\n",
        "test = sum_columns_and_delete(test,cols,'cuisines')\n",
        "\n",
        "cols=['dish_liked_0','dish_liked_1','dish_liked_2','dish_liked_3','dish_liked_4','dish_liked_5','dish_liked_6']\n",
        "train_encoded = sum_columns_and_delete(train_encoded,cols,'dish_liked')\n",
        "test = sum_columns_and_delete(test,cols,'dish_liked')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFrQxQ3J_Wcy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(train_encoded.corr(),annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = train_encoded.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(train_encoded.values, i) for i in range(train_encoded.shape[1])]\n",
        "\n",
        "vif_data.set_index('feature')"
      ],
      "metadata": {
        "id": "73o49tLMnQm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktBEgDJmhD6u"
      },
      "outputs": [],
      "source": [
        "train_encoded.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfnZvCVbcPWz"
      },
      "outputs": [],
      "source": [
        "cols = ['rest_type','cuisines','approx_cost(for two people)','classes','num_spec','num_cuisines','num_dish_liked','listed_in(type)','listed_in(city)','num_reviews','book_table','online_order','votes','is_rate_valid']\n",
        "train_encoded = train_encoded[cols]\n",
        "test = test[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6VB_B54P6Jr"
      },
      "outputs": [],
      "source": [
        "X_train,y_train = train_encoded.drop('approx_cost(for two people)',axis=1),train_encoded['approx_cost(for two people)']\n",
        "X_test,y_test = test.drop('approx_cost(for two people)',axis=1),test['approx_cost(for two people)']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHHjkuLzUMjt"
      },
      "outputs": [],
      "source": [
        "sns.histplot(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjjwS4gZUho3"
      },
      "outputs": [],
      "source": [
        "#y_train = np.log(y_train)\n",
        "#y_test = np.log(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt54TGREWJuG"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9YvlMvJWbY4"
      },
      "outputs": [],
      "source": [
        "def predict(ml_model,X_train=X_train,X_test=X_test):\n",
        "    #this fucntion helps to train the model and plot the results\n",
        "\n",
        "  model = ml_model.fit(X_train,y_train)\n",
        "  print('Training score : {}'.format(model.score(X_train,y_train)))\n",
        "  y_prediction = model.predict(X_test)\n",
        "  print('Predictions are : {}'.format(y_prediction))\n",
        "  print('\\n')\n",
        "  r2_score = metrics.r2_score(y_test, y_prediction)\n",
        "  print('r2_score : {}'.format(r2_score))\n",
        "  print('MAE : {}'.format(metrics.mean_absolute_error(y_test, y_prediction)))\n",
        "  print('MSE : {}'.format(metrics.mean_squared_error(y_test, y_prediction)))\n",
        "  print('RMSE : {}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_prediction))))\n",
        "\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
        "  sns.distplot(y_test-y_prediction,ax=ax1)\n",
        "  ax1.set_title('Distribution of Prediction Errors')\n",
        "  #sns.distplot(y_test-y_prediction)\n",
        "  ax2.scatter(y_test, y_prediction, color = 'blue')\n",
        "  ax2.plot(y_prediction, y_prediction, color = 'red')\n",
        "  ax2.set_xlabel('Predicted')\n",
        "  ax2.set_ylabel('Actual')\n",
        "  ax2.set_title('Actual vs Predicted')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JYaaeKAWmxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "import joblib\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=600, random_state=42)\n",
        "\n",
        "predict(rf_reg,X_train=X_train,X_test=X_test)\n",
        "joblib.dump(rf_reg, 'random_forest_model.pkl')  # Save the model to a file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maUtFyc4-M73"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined and preprocessed\n",
        "\n",
        "# Convert datasets into DMatrix, the data structure used by XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define XGBoost parameters (these can be tuned further)\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression tasks\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'alpha': 10,\n",
        "    'n_estimators': 100,\n",
        "    'eval_metric': 'rmse'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "bst = xgb.train(params, dtrain, num_boost_round=2000, evals=[(dtest, \"test\")], early_stopping_rounds=10, verbose_eval=True)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bst.predict(dtest)\n",
        "\n",
        "# Evaluate the model\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"RMSE: {rmse}\")\n",
        "bst.save_model('xgb_model.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoMvweK1-snS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "y_pred = bst.predict(dtest)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
        "print(f'R-squared (R2): {r2:.5f}')\n",
        "\n",
        "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
        "sns.distplot(y_test-y_pred.reshape(-1),ax=ax1)\n",
        "ax1.set_title('Distribution of Prediction Errors')\n",
        "#sns.distplot(y_test-y_pred)\n",
        "ax2.scatter(y_test, y_pred.reshape(-1), color = 'blue')\n",
        "ax2.plot(y_pred.reshape(-1), y_pred.reshape(-1), color = 'red')\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_ylabel('Actual')\n",
        "ax2.set_title('Actual vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUP_XVcIZ5E5"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JLbfzC8YXeI1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adam, Nadam, Adadelta,AdamW,Adamax\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "# Choose the optimizer\n",
        "optimizer = AdamW(learning_rate=0.1, epsilon=1e-7)\n",
        "\n",
        "# Define your model architecture\n",
        "ann = tf.keras.models.Sequential()\n",
        "ann.add(Dense(units=13, activation='tanh'))\n",
        "ann.add(BatchNormalization())\n",
        "ann.add(Dropout(0.18))\n",
        "\n",
        "ann.add(Dense(units=32, activation=None))\n",
        "ann.add(LeakyReLU(alpha=0.01))\n",
        "ann.add(BatchNormalization())\n",
        "ann.add(Dropout(0.18))\n",
        "\n",
        "ann.add(Dense(units=64, activation=None))\n",
        "ann.add(LeakyReLU(alpha=0.01))\n",
        "ann.add(BatchNormalization())\n",
        "ann.add(Dropout(0.18))\n",
        "\n",
        "ann.add(Dense(units=32, activation=None))\n",
        "ann.add(LeakyReLU(alpha=0.01))\n",
        "ann.add(BatchNormalization())\n",
        "ann.add(Dropout(0.18))\n",
        "\n",
        "ann.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "\n",
        "# Callback to reduce learning rate on plateau\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  factor=0.9,\n",
        "                                  patience=5,\n",
        "                                  min_lr=1e-6,\n",
        "                                  verbose=1)\n",
        "\n",
        "# Callback to save the best model\n",
        "checkpoint = ModelCheckpoint(filepath='best_ann_model.keras',\n",
        "                             monitor='val_loss',\n",
        "                             save_best_only=True,\n",
        "                             mode='min',\n",
        "                             verbose=1)\n",
        "\n",
        "ann.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Fit the model with callbacks\n",
        "ann.fit(X_train, y_train,\n",
        "        batch_size=1024,\n",
        "        epochs=100000,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=[early_stop, lr_reduction, checkpoint],\n",
        "        verbose=1)\n",
        "\n",
        "# Load the best model\n",
        "ann.load_weights('best_ann_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXvHS0FcoP7I"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=ann.history.epoch[50:],y=ann.history.history['loss'][50:],label='Training Loss')\n",
        "sns.lineplot(x=ann.history.epoch[50:],y=ann.history.history['val_loss'][50:],label='Validation Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dde6w2WcY39a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "y_pred = ann.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
        "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
        "print(f'R-squared (R2): {r2:.5f}')\n",
        "\n",
        "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
        "sns.distplot(y_test-y_pred.reshape(-1),ax=ax1)\n",
        "ax1.set_title('Distribution of Prediction Errors')\n",
        "#sns.distplot(y_test-y_pred)\n",
        "ax2.scatter(y_test, y_pred.reshape(-1), color = 'blue')\n",
        "ax2.plot(y_pred.reshape(-1), y_pred.reshape(-1), color = 'red')\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_ylabel('Actual')\n",
        "ax2.set_title('Actual vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvement\n",
        "\n",
        "## 1. Increase Review Volume and Enhance Aspect Analysis\n",
        "- Gather more reviews and utilize advanced aspect analysis using a paid LLM model. This will add more complexity to the model, leading to more accurate predictions.\n",
        "\n",
        "## 2. Improve Address Accuracy\n",
        "- Obtain more precise addresses or correct existing ones. This will help reveal detailed location information, which can significantly impact price predictions.\n",
        "\n",
        "## 3. Incorporate Detailed Menu Pricing\n",
        "- Instead of predicting an approximation for the entire menu, obtain detailed menus with item-specific prices. This approach will enhance the accuracy of price predictions.\n",
        "\n",
        "## 4. Expand Data Collection\n",
        "- Collect additional data to improve the overall model performance and prediction accuracy.\n"
      ],
      "metadata": {
        "id": "PpyJQZK5MAMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html '/content/drive/MyDrive/Zomato Geospatial Analysis/Copy of Bangalore Dining Insights & Strategic Investment Plan + Price Prediction with ML.ipynb'"
      ],
      "metadata": {
        "id": "30z32oOX2GEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kxG4CfVOCLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}